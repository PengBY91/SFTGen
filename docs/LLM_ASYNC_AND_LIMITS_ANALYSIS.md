# å¤§æ¨¡å‹è°ƒç”¨å¼‚æ­¥ä¸å¹¶å‘é™åˆ¶åˆ†ææŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†ä»£ç åº“ä¸­æ¶‰åŠå¤§æ¨¡å‹è°ƒç”¨çš„æ‰€æœ‰ä»£ç ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. **æ˜¯å¦ä¸ºå¼‚æ­¥è¯·æ±‚**
2. **æœ€å¤§è¯·æ±‚å¹¶å‘æ•°é‡æ˜¯å¦æœ‰é™åˆ¶**

## âœ… 1. å¼‚æ­¥è¯·æ±‚åˆ†æ

### 1.1 åŸºç¡€æ¶æ„ï¼šå®Œå…¨å¼‚æ­¥å®ç°

**æ‰€æœ‰LLMè°ƒç”¨éƒ½æ˜¯å¼‚æ­¥çš„**ï¼Œä»£ç åº“é‡‡ç”¨äº†å…¨é¢çš„å¼‚æ­¥æ¶æ„ï¼š

#### åŸºç¡€æŠ½è±¡ç±»
```39:44:graphgen/bases/base_llm_client.py
    @abc.abstractmethod
    async def generate_answer(
        self, text: str, history: Optional[List[str]] = None, **extra: Any
    ) -> str:
        """Generate answer from the model."""
        raise NotImplementedError
```

æ‰€æœ‰æ ¸å¿ƒæ–¹æ³•éƒ½å®šä¹‰ä¸º `async def`ï¼Œå¼ºåˆ¶è¦æ±‚å¼‚æ­¥å®ç°ã€‚

#### å…·ä½“å®ç°ï¼šOpenAIå®¢æˆ·ç«¯
```148:176:graphgen/models/llm/openai_client.py
    async def generate_answer(
        self,
        text: str,
        history: Optional[List[str]] = None,
        **extra: Any,
    ) -> str:
        kwargs = self._pre_generate(text, history)

        prompt_tokens = 0
        for message in kwargs["messages"]:
            prompt_tokens += len(self.tokenizer.encode(message["content"]))
        estimated_tokens = prompt_tokens + kwargs["max_tokens"]

        if self.request_limit:
            await self.rpm.wait(silent=True)
            await self.tpm.wait(estimated_tokens, silent=True)

        completion = await self.client.chat.completions.create(  # pylint: disable=E1125
            model=self.model_name, **kwargs
        )
        if hasattr(completion, "usage"):
            self.token_usage.append(
                {
                    "prompt_tokens": completion.usage.prompt_tokens,
                    "completion_tokens": completion.usage.completion_tokens,
                    "total_tokens": completion.usage.total_tokens,
                }
            )
        return self.filter_think_tags(completion.choices[0].message.content)
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `AsyncOpenAI` å®¢æˆ·ç«¯ï¼ˆç¬¬63è¡Œï¼‰
- ä½¿ç”¨ `await` å…³é”®å­—è°ƒç”¨ APIï¼ˆç¬¬165è¡Œï¼‰
- å¼‚æ­¥é™æµç­‰å¾…ï¼ˆç¬¬162-163è¡Œï¼‰

#### å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ
```67:87:graphgen/models/llm/openai_client.py
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºï¼Œç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®å…³é—­"""
        await self.aclose()
        return False
    
    async def aclose(self):
        """å…³é—­å¼‚æ­¥å®¢æˆ·ç«¯"""
        try:
            if hasattr(self, 'client') and self.client is not None:
                await self.client.close()
        except RuntimeError as e:
            # å¿½ç•¥"Event loop is closed"é”™è¯¯
            if "Event loop is closed" not in str(e):
                raise
        except Exception:
            # é™é»˜å¤„ç†å…¶ä»–å…³é—­é”™è¯¯
            pass
```

### 1.2 å¼‚æ­¥è°ƒç”¨é“¾

#### çŸ¥è¯†å›¾è°±æŠ½å–
```44:50:graphgen/operators/build_kg/build_text_kg.py
    results = await run_concurrent(
        kg_builder.extract,
        chunks,
        desc="[2/4]Extracting entities and relationships from chunks",
        unit="chunk",
        progress_bar=progress_bar,
    )
```

#### å®é™…æå–æ–¹æ³•
```44:76:graphgen/models/kg_builder/light_rag_kg_builder.py
    async def extract(
        self, chunk: Chunk
    ) -> Tuple[Dict[str, List[dict]], Dict[Tuple[str, str], List[dict]]]:
        """
        Extract entities and relationships from a single chunk using the LLM client.
        Supports caching to avoid re-extraction of identical chunks.
        :param chunk
        :return: (nodes_data, edges_data)
        """
        chunk_id = chunk.id
        content = chunk.content
        
        # Check cache first if enabled
        if self.enable_cache:
            chunk_hash = compute_content_hash(content, prefix="extract-")
            cached_result = await self.cache_storage.get_by_id(chunk_hash)
            if cached_result is not None:
                logger.debug("Cache hit for chunk %s", chunk_id)
                return cached_result["nodes"], cached_result["edges"]

        # step 1: language_detection
        language = detect_main_language(content)

        hint_prompt = KG_EXTRACTION_PROMPT[language]["TEMPLATE"].format(
            **KG_EXTRACTION_PROMPT["FORMAT"], input_text=content
        )

        # step 2: initial glean
        if self.batch_manager:
            final_result = await self.batch_manager.add_request(hint_prompt)
        else:
            final_result = await self.llm_client.generate_answer(hint_prompt)
```

**æ€»ç»“**ï¼šæ•´ä¸ªè°ƒç”¨é“¾éƒ½æ˜¯å¼‚æ­¥çš„ï¼Œä»é¡¶å±‚çš„ `run_concurrent` åˆ°åº•å±‚çš„ API è°ƒç”¨ã€‚

---

## âš ï¸ 2. å¹¶å‘é™åˆ¶åˆ†æ

### 2.1 å¹¶å‘é™åˆ¶æœºåˆ¶å­˜åœ¨ï¼Œä½†**é»˜è®¤æœªå¯ç”¨**

#### 2.1.1 `run_concurrent` å‡½æ•°çš„å¹¶å‘é™åˆ¶

```83:106:graphgen/utils/run_concurrent.py
async def run_concurrent(
    coro_fn: Callable[[T], Awaitable[R]],
    items: List[T],
    *,
    desc: str = "processing",
    unit: str = "item",
    progress_bar: Optional[Any] = None,
    log_interval: int = 50,  # é»˜è®¤æ¯ 50 ä¸ªè®°å½•ä¸€æ¬¡æ—¥å¿—
    desc_callback: Optional[Callable[[int, int, List[R]], str]] = None,  # æ–°å¢ï¼šåŠ¨æ€æè¿°å›è°ƒ (completed_count, total, results) -> desc
    max_concurrent: Optional[int] = None,  # æ–°å¢ï¼šæœ€å¤§å¹¶å‘æ•°ï¼ŒNone è¡¨ç¤ºæ— é™åˆ¶
) -> List[R]:
    import time
    
    # å¦‚æœæœ‰å¹¶å‘é™åˆ¶ï¼Œä½¿ç”¨ Semaphore åŒ…è£… coro_fn
    if max_concurrent is not None and max_concurrent > 0:
        semaphore = asyncio.Semaphore(max_concurrent)
        original_coro_fn = coro_fn
        
        async def limited_coro_fn(item: T) -> R:
            async with semaphore:
                return await original_coro_fn(item)
        
        coro_fn = limited_coro_fn
        logger.debug(f"å¯ç”¨å¹¶å‘é™åˆ¶: max_concurrent={max_concurrent}")
```

**å…³é”®å‘ç°**ï¼š
- âœ… æ”¯æŒ `max_concurrent` å‚æ•°
- âš ï¸ **é»˜è®¤å€¼ä¸º `None`ï¼ˆæ— é™åˆ¶ï¼‰**
- âœ… å¦‚æœè®¾ç½®ï¼Œä¼šä½¿ç”¨ `asyncio.Semaphore` è¿›è¡Œé™åˆ¶

#### 2.1.2 å®é™…è°ƒç”¨æƒ…å†µ

**æ‰€æœ‰å®é™…è°ƒç”¨ä¸­ï¼Œéƒ½æ²¡æœ‰ä¼ å…¥ `max_concurrent` å‚æ•°**ï¼š

```44:50:graphgen/operators/build_kg/build_text_kg.py
    results = await run_concurrent(
        kg_builder.extract,
        chunks,
        desc="[2/4]Extracting entities and relationships from chunks",
        unit="chunk",
        progress_bar=progress_bar,
    )
```

è¿™æ„å‘³ç€**å½“å‰æ‰€æœ‰LLMè°ƒç”¨éƒ½æ²¡æœ‰å¹¶å‘æ•°é‡é™åˆ¶**ã€‚

### 2.2 æ‰¹é‡è¯·æ±‚ç®¡ç†å™¨çš„å¹¶å‘é™åˆ¶

```24:62:graphgen/utils/batch_request_manager.py
class BatchRequestManager:
    """
    æ‰¹é‡è¯·æ±‚ç®¡ç†å™¨
    æ”¶é›†å¤šä¸ªè¯·æ±‚ï¼Œæ‰¹é‡å¹¶å‘å¤„ç†ï¼Œå‡å°‘ç½‘ç»œå»¶è¿Ÿ
    """
    
    def __init__(
        self,
        llm_client,
        batch_size: int = 10,
        max_wait_time: float = 0.5,
        enable_batching: bool = True,
        max_concurrent: Optional[int] = None,  # æ–°å¢ï¼šæœ€å¤§å¹¶å‘æ•°ï¼ŒNone è¡¨ç¤ºæ— é™åˆ¶
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡è¯·æ±‚ç®¡ç†å™¨
        
        :param llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        :param batch_size: æ¯æ‰¹å¤„ç†çš„è¯·æ±‚æ•°é‡
        :param max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´å³ä½¿æœªè¾¾åˆ°batch_sizeä¹Ÿä¼šå‘é€
        :param enable_batching: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†
        :param max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œç”¨äºé™åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡ï¼ˆé€‚ç”¨äº Ollama ç­‰æœåŠ¡ï¼‰
        """
        self.llm_client = llm_client
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.enable_batching = enable_batching
        self.max_concurrent = max_concurrent
        
        self.request_queue: List[BatchRequest] = []
        self.queue_lock = asyncio.Lock()
        self.batch_task: Optional[asyncio.Task] = None
        self.pending_futures: Dict[int, asyncio.Future] = {}
        self.request_counter = 0
        
        # å¦‚æœæœ‰å¹¶å‘é™åˆ¶ï¼Œåˆ›å»º Semaphore
        self.semaphore = asyncio.Semaphore(max_concurrent) if max_concurrent and max_concurrent > 0 else None
        if self.semaphore:
            logger.debug(f"BatchRequestManager å¯ç”¨å¹¶å‘é™åˆ¶: max_concurrent={max_concurrent}")
```

**å…³é”®å‘ç°**ï¼š
- âœ… æ”¯æŒ `max_concurrent` å‚æ•°
- âš ï¸ **é»˜è®¤å€¼ä¸º `None`ï¼ˆæ— é™åˆ¶ï¼‰**

#### å®é™…åˆ›å»ºæ—¶çš„è°ƒç”¨

```34:42:graphgen/models/kg_builder/light_rag_kg_builder.py
        self.enable_batch_requests = enable_batch_requests
        self.batch_manager: Optional[BatchRequestManager] = None
        if enable_batch_requests:
            self.batch_manager = BatchRequestManager(
                llm_client=llm_client,
                batch_size=batch_size,
                max_wait_time=max_wait_time,
                enable_batching=True
            )
```

**å…³é”®å‘ç°**ï¼šåˆ›å»º `BatchRequestManager` æ—¶**æ²¡æœ‰ä¼ å…¥ `max_concurrent` å‚æ•°**ï¼Œæ„å‘³ç€ä½¿ç”¨é»˜è®¤å€¼ `None`ï¼ˆæ— é™åˆ¶ï¼‰ã€‚

### 2.3 é€Ÿç‡é™åˆ¶ï¼ˆRPM/TPMï¼‰

è™½ç„¶å¹¶å‘æ•°é‡æ²¡æœ‰é™åˆ¶ï¼Œä½†æœ‰**é€Ÿç‡é™åˆ¶æœºåˆ¶**ï¼š

#### RPMï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰é™åˆ¶

```8:44:graphgen/models/llm/limitter.py
class RPM:
    def __init__(self, rpm: int = 1000):
        self.rpm = rpm
        self.record = {"rpm_slot": self.get_minute_slot(), "counter": 0}

    @staticmethod
    def get_minute_slot():
        current_time = time.time()
        dt_object = datetime.fromtimestamp(current_time)
        total_minutes_since_midnight = dt_object.hour * 60 + dt_object.minute
        return total_minutes_since_midnight

    async def wait(self, silent=False):
        current = time.time()
        dt_object = datetime.fromtimestamp(current)
        minute_slot = self.get_minute_slot()

        if self.record["rpm_slot"] == minute_slot:
            # check RPM exceed
            if self.record["counter"] >= self.rpm:
                # wait until next minute
                next_minute = dt_object.replace(second=0, microsecond=0) + timedelta(
                    minutes=1
                )
                _next = next_minute.timestamp()
                sleep_time = abs(_next - current)
                if not silent:
                    logger.info("RPM sleep %s", sleep_time)
                await asyncio.sleep(sleep_time)

                self.record = {"rpm_slot": self.get_minute_slot(), "counter": 0}
        else:
            self.record = {"rpm_slot": self.get_minute_slot(), "counter": 0}
        self.record["counter"] += 1

        if not silent:
            logger.debug(self.record)
```

#### TPMï¼ˆæ¯åˆ†é’ŸTokenæ•°ï¼‰é™åˆ¶

```47:86:graphgen/models/llm/limitter.py
class TPM:
    def __init__(self, tpm: int = 20000):
        self.tpm = tpm
        self.record = {"tpm_slot": self.get_minute_slot(), "counter": 0}

    @staticmethod
    def get_minute_slot():
        current_time = time.time()
        dt_object = datetime.fromtimestamp(current_time)
        total_minutes_since_midnight = dt_object.hour * 60 + dt_object.minute
        return total_minutes_since_midnight

    async def wait(self, token_count, silent=False):
        current = time.time()
        dt_object = datetime.fromtimestamp(current)
        minute_slot = self.get_minute_slot()

        # get next slot, skip
        if self.record["tpm_slot"] != minute_slot:
            self.record = {"tpm_slot": minute_slot, "counter": token_count}
            return

        # check RPM exceed
        old_counter = self.record["counter"]
        self.record["counter"] += token_count
        if self.record["counter"] > self.tpm:
            logger.info("Current TPM: %s, limit: %s", old_counter, self.tpm)
            # wait until next minute
            next_minute = dt_object.replace(second=0, microsecond=0) + timedelta(
                minutes=1
            )
            _next = next_minute.timestamp()
            sleep_time = abs(_next - current)
            logger.warning("TPM limit exceeded, wait %s seconds", sleep_time)
            await asyncio.sleep(sleep_time)

            self.record = {"tpm_slot": self.get_minute_slot(), "counter": token_count}

        if not silent:
            logger.debug(self.record)
```

#### é€Ÿç‡é™åˆ¶çš„ä½¿ç”¨

```161:163:graphgen/models/llm/openai_client.py
        if self.request_limit:
            await self.rpm.wait(silent=True)
            await self.tpm.wait(estimated_tokens, silent=True)
```

**å…³é”®å‘ç°**ï¼š
- âœ… æœ‰RPMï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰å’ŒTPMï¼ˆæ¯åˆ†é’ŸTokenæ•°ï¼‰é™åˆ¶
- âš ï¸ **åªæœ‰å½“ `request_limit=True` æ—¶æ‰ä¼šå¯ç”¨**
- âš ï¸ é»˜è®¤å€¼ï¼šRPM=1000ï¼ŒTPM=50000

#### å®é™…é…ç½®

åœ¨ä»»åŠ¡å¤„ç†å™¨ä¸­å¯ä»¥çœ‹åˆ°ï¼š

```68:76:webui/task_processor.py
            synthesizer_llm_client = OpenAIClient(
                model_name=env.get("SYNTHESIZER_MODEL", ""),
                base_url=env.get("SYNTHESIZER_BASE_URL", ""),
                api_key=env.get("SYNTHESIZER_API_KEY", ""),
                request_limit=True,
                rpm=RPM(env.get("RPM", 1000)),
                tpm=TPM(env.get("TPM", 50000)),
                tokenizer=tokenizer_instance,
            )
```

**å…³é”®å‘ç°**ï¼šåœ¨æŸäº›åœºæ™¯ä¸‹ä¼šè®¾ç½® `request_limit=True` å¹¶é…ç½®RPM/TPMï¼Œä½†è¿™**ä¸æ˜¯å¹¶å‘æ•°é‡é™åˆ¶**ï¼Œè€Œæ˜¯**é€Ÿç‡é™åˆ¶**ã€‚

---

## ğŸ“Š 3. æ€»ç»“ä¸é£é™©è¯„ä¼°

### 3.1 å¼‚æ­¥è¯·æ±‚ï¼šâœ… å®Œå…¨å¼‚æ­¥

**ç»“è®º**ï¼šæ‰€æœ‰å¤§æ¨¡å‹è°ƒç”¨éƒ½æ˜¯å¼‚æ­¥çš„ï¼ŒåŒ…æ‹¬ï¼š
- åŸºç¡€æŠ½è±¡ç±»ä½¿ç”¨ `async def`
- å…·ä½“å®ç°ä½¿ç”¨ `AsyncOpenAI` å®¢æˆ·ç«¯
- æ‰€æœ‰è°ƒç”¨ä½¿ç”¨ `await` å…³é”®å­—
- å®Œæ•´çš„å¼‚æ­¥è°ƒç”¨é“¾

### 3.2 å¹¶å‘é™åˆ¶ï¼šâš ï¸ é»˜è®¤æ— é™åˆ¶

#### å½“å‰çŠ¶æ€

| é™åˆ¶ç±»å‹ | æ˜¯å¦å­˜åœ¨ | æ˜¯å¦é»˜è®¤å¯ç”¨ | é»˜è®¤å€¼ | å®é™…ä½¿ç”¨æƒ…å†µ |
|---------|---------|-------------|--------|-------------|
| **å¹¶å‘æ•°é‡é™åˆ¶** | âœ… æ˜¯ | âŒ å¦ | `None` (æ— é™åˆ¶) | æ‰€æœ‰è°ƒç”¨éƒ½æœªè®¾ç½® |
| **é€Ÿç‡é™åˆ¶ (RPM)** | âœ… æ˜¯ | âš ï¸ æ¡ä»¶å¯ç”¨ | 1000/åˆ†é’Ÿ | éœ€è¦ `request_limit=True` |
| **é€Ÿç‡é™åˆ¶ (TPM)** | âœ… æ˜¯ | âš ï¸ æ¡ä»¶å¯ç”¨ | 50000/åˆ†é’Ÿ | éœ€è¦ `request_limit=True` |

#### é£é™©åˆ†æ

1. **é«˜å¹¶å‘é£é™©**ï¼š
   - å¦‚æœå¤„ç†å¤§é‡ chunksï¼ˆä¾‹å¦‚1000+ï¼‰ï¼Œä¼šåŒæ—¶å‘èµ·1000+ä¸ªå¹¶å‘è¯·æ±‚
   - å¯èƒ½å¯¼è‡´APIæœåŠ¡å™¨è¿‡è½½
   - å¯èƒ½è§¦å‘APIæä¾›å•†çš„é™æµæœºåˆ¶
   - å¯èƒ½å¯¼è‡´æœ¬åœ°èµ„æºè€—å°½ï¼ˆå†…å­˜ã€è¿æ¥æ•°ï¼‰

2. **APIé™æµé£é™©**ï¼š
   - å¤§å¤šæ•°APIæä¾›å•†éƒ½æœ‰å¹¶å‘é™åˆ¶ï¼ˆä¾‹å¦‚OpenAIï¼š10-50ä¸ªå¹¶å‘ï¼‰
   - æ— é™åˆ¶å¹¶å‘å¯èƒ½å¯¼è‡´è¯·æ±‚è¢«æ‹’ç»æˆ–è¿”å›429é”™è¯¯

3. **èµ„æºè€—å°½é£é™©**ï¼š
   - å¤§é‡å¹¶å‘è¿æ¥å¯èƒ½å¯¼è‡´ï¼š
     - å†…å­˜å ç”¨è¿‡é«˜
     - æ–‡ä»¶æè¿°ç¬¦è€—å°½
     - ç½‘ç»œè¿æ¥æ•°è€—å°½

---

## ğŸ”§ 4. å»ºè®®ä¸æ”¹è¿›æ–¹æ¡ˆ

### 4.1 ç«‹å³å»ºè®®ï¼šæ·»åŠ é»˜è®¤å¹¶å‘é™åˆ¶

#### å»ºè®®1ï¼šä¸º `run_concurrent` æ·»åŠ é»˜è®¤å¹¶å‘é™åˆ¶

```python
# å½“å‰ä»£ç ï¼ˆæ— é™åˆ¶ï¼‰
results = await run_concurrent(
    kg_builder.extract,
    chunks,
    desc="[2/4]Extracting entities",
)

# å»ºè®®ä¿®æ”¹ï¼ˆæ·»åŠ å¹¶å‘é™åˆ¶ï¼‰
results = await run_concurrent(
    kg_builder.extract,
    chunks,
    desc="[2/4]Extracting entities",
    max_concurrent=50,  # æ ¹æ®APIæä¾›å•†è®¾ç½®åˆé€‚çš„å€¼
)
```

#### å»ºè®®2ï¼šä¸º `BatchRequestManager` æ·»åŠ é»˜è®¤å¹¶å‘é™åˆ¶

```python
# å½“å‰ä»£ç ï¼ˆæ— é™åˆ¶ï¼‰
self.batch_manager = BatchRequestManager(
    llm_client=llm_client,
    batch_size=batch_size,
    max_wait_time=max_wait_time,
    enable_batching=True
)

# å»ºè®®ä¿®æ”¹ï¼ˆæ·»åŠ å¹¶å‘é™åˆ¶ï¼‰
self.batch_manager = BatchRequestManager(
    llm_client=llm_client,
    batch_size=batch_size,
    max_wait_time=max_wait_time,
    enable_batching=True,
    max_concurrent=50,  # æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
)
```

### 4.2 é…ç½®åŒ–å¹¶å‘é™åˆ¶

å»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ å¹¶å‘é™åˆ¶é…ç½®ï¼š

```yaml
# å»ºè®®é…ç½®é¡¹
llm_config:
  max_concurrent_requests: 50  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
  rpm: 1000  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
  tpm: 50000  # æ¯åˆ†é’ŸTokenæ•°
```

### 4.3 æ ¹æ®APIæä¾›å•†è‡ªåŠ¨è®¾ç½®

ä¸åŒAPIæä¾›å•†çš„å¹¶å‘é™åˆ¶ä¸åŒï¼š

| APIæä¾›å•† | å…¸å‹å¹¶å‘é™åˆ¶ | å»ºè®®é…ç½® |
|----------|------------|---------|
| OpenAI (ä»˜è´¹è´¦æˆ·) | 50-200 | 50 |
| OpenAI (å…è´¹è´¦æˆ·) | 5-10 | 5 |
| æœ¬åœ°éƒ¨ç½² (Ollama) | 1-4 | 2-4 |
| å…¶ä»–äº‘æœåŠ¡ | 10-50 | 30 |

---

## ğŸ“ 5. ä»£ç ç¤ºä¾‹ï¼šå¦‚ä½•å¯ç”¨å¹¶å‘é™åˆ¶

### ç¤ºä¾‹1ï¼šåœ¨çŸ¥è¯†å›¾è°±æŠ½å–æ—¶å¯ç”¨å¹¶å‘é™åˆ¶

```python
# åœ¨ graphgen/operators/build_kg/build_text_kg.py ä¸­
results = await run_concurrent(
    kg_builder.extract,
    chunks,
    desc="[2/4]Extracting entities and relationships from chunks",
    unit="chunk",
    progress_bar=progress_bar,
    max_concurrent=50,  # æ·»åŠ æ­¤å‚æ•°
)
```

### ç¤ºä¾‹2ï¼šåœ¨æ‰¹é‡è¯·æ±‚ç®¡ç†å™¨ä¸­å¯ç”¨å¹¶å‘é™åˆ¶

```python
# åœ¨ graphgen/models/kg_builder/light_rag_kg_builder.py ä¸­
if enable_batch_requests:
    self.batch_manager = BatchRequestManager(
        llm_client=llm_client,
        batch_size=batch_size,
        max_wait_time=max_wait_time,
        enable_batching=True,
        max_concurrent=50,  # æ·»åŠ æ­¤å‚æ•°
    )
```

---

## âœ… 6. ç»“è®º

### å¼‚æ­¥è¯·æ±‚
- âœ… **æ‰€æœ‰LLMè°ƒç”¨éƒ½æ˜¯å¼‚æ­¥çš„**
- âœ… ä½¿ç”¨äº†å®Œæ•´çš„å¼‚æ­¥æ¶æ„
- âœ… æ€§èƒ½è‰¯å¥½

### å¹¶å‘é™åˆ¶
- âš ï¸ **å½“å‰é»˜è®¤æ— å¹¶å‘æ•°é‡é™åˆ¶**
- âš ï¸ **å­˜åœ¨é«˜å¹¶å‘é£é™©**
- âœ… æœ‰é€Ÿç‡é™åˆ¶æœºåˆ¶ï¼ˆRPM/TPMï¼‰ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¯ç”¨
- âœ… ä»£ç å·²æ”¯æŒå¹¶å‘é™åˆ¶ï¼Œä½†æœªåœ¨å®é™…ä½¿ç”¨ä¸­å¯ç”¨

### å»ºè®®è¡ŒåŠ¨
1. **ç«‹å³**ï¼šåœ¨æ‰€æœ‰ `run_concurrent` è°ƒç”¨ä¸­æ·»åŠ  `max_concurrent` å‚æ•°
2. **çŸ­æœŸ**ï¼šåœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ å¹¶å‘é™åˆ¶é…ç½®é¡¹
3. **ä¸­æœŸ**ï¼šæ ¹æ®APIæä¾›å•†è‡ªåŠ¨è®¾ç½®åˆé€‚çš„å¹¶å‘é™åˆ¶
4. **é•¿æœŸ**ï¼šæ·»åŠ åŠ¨æ€å¹¶å‘è°ƒæ•´æœºåˆ¶ï¼Œæ ¹æ®APIå“åº”æ—¶é—´è‡ªåŠ¨ä¼˜åŒ–

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**ï¼š2025-01-27
**åˆ†æèŒƒå›´**ï¼šæ•´ä¸ªä»£ç åº“ä¸­æ¶‰åŠLLMè°ƒç”¨çš„æ‰€æœ‰ä»£ç 

