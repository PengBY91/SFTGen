#!/usr/bin/env python3
"""
KGE-Gen å‘½ä»¤è¡Œå·¥å…·
å°† KGE-Gen Demo web app è½¬æ¢ä¸ºå‘½ä»¤è¡Œè„šæœ¬ç‰ˆæœ¬
"""

import argparse
import json
import logging
import os
import sys
import time
from datetime import datetime
from importlib.resources import files

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

from graphgen.graphgen import GraphGen
from graphgen.models import OpenAIClient, Tokenizer
from graphgen.models.llm.limitter import RPM, TPM
from graphgen.utils import set_logger
from webui.utils import cleanup_workspace, setup_workspace


class GraphGenCLI:
    """KGE-Gen å‘½ä»¤è¡Œæ¥å£ç±»"""
    
    def __init__(self):
        self.root_dir = files("webui").parent
        sys.path.append(self.root_dir)
        load_dotenv()
        self.batch_logger = None
        self.batch_stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "total_tokens": 0,
            "total_time": 0,
            "file_stats": []
        }
        
    def test_api_connection(self, api_base: str, api_key: str, model_name: str) -> bool:
        """æµ‹è¯• API è¿æ¥"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=api_key, base_url=api_base)
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1
            )
            if not response.choices or not response.choices[0].message:
                print(f"âŒ {model_name}: API å“åº”æ— æ•ˆ")
                return False
            print(f"âœ… {model_name}: API è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ {model_name}: API è¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def init_graph_gen(self, config: dict, env: dict) -> GraphGen:
        """åˆå§‹åŒ– KGE-Gen å®ä¾‹"""
        # è®¾ç½®å·¥ä½œç›®å½•
        log_file, working_dir = setup_workspace(os.path.join(self.root_dir, "cache"))
        set_logger(log_file, if_stream=True)
        os.environ.update({k: str(v) for k, v in env.items()})

        # åˆ›å»º tokenizer å’Œ LLM å®¢æˆ·ç«¯
        tokenizer_instance = Tokenizer(config.get("tokenizer", "cl100k_base"))
        synthesizer_llm_client = OpenAIClient(
            model_name=env.get("SYNTHESIZER_MODEL", ""),
            base_url=env.get("SYNTHESIZER_BASE_URL", ""),
            api_key=env.get("SYNTHESIZER_API_KEY", ""),
            request_limit=True,
            rpm=RPM(env.get("RPM", 1000)),
            tpm=TPM(env.get("TPM", 50000)),
            tokenizer=tokenizer_instance,
        )

        trainee_llm_client = OpenAIClient(
            model_name=env.get("TRAINEE_MODEL", ""),
            base_url=env.get("TRAINEE_BASE_URL", ""),
            api_key=env.get("TRAINEE_API_KEY", ""),
            request_limit=True,
            rpm=RPM(env.get("RPM", 1000)),
            tpm=TPM(env.get("TPM", 50000)),
            tokenizer=tokenizer_instance,
        )

        # åˆ›å»º KGE-Gen å®ä¾‹ï¼ˆä¼ é€’ synthesizer_llm_client å’Œ trainee_llm_clientï¼‰
        graph_gen = GraphGen(
            working_dir=working_dir,
            tokenizer_instance=tokenizer_instance,
            synthesizer_llm_client=synthesizer_llm_client,
            trainee_llm_client=trainee_llm_client,
        )

        return graph_gen
    
    def count_tokens(self, file_path: str, tokenizer_name: str) -> tuple:
        """è®¡ç®—æ–‡ä»¶ä¸­çš„ token æ•°é‡"""
        if not file_path or not os.path.exists(file_path):
            return 0, 0

        if file_path.endswith(".jsonl"):
            with open(file_path, "r", encoding="utf-8") as f:
                data = [json.loads(line) for line in f]
        elif file_path.endswith(".json"):
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                data = [item for sublist in data for item in sublist]
        elif file_path.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as f:
                data = f.read()
                chunks = [data[i : i + 512] for i in range(0, len(data), 512)]
                data = [{"content": chunk} for chunk in chunks]
        elif file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
            if "content" in df.columns:
                data = df["content"].tolist()
            else:
                data = df.iloc[:, 0].tolist()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")

        tokenizer = Tokenizer(tokenizer_name)

        # è®¡ç®— token æ•°é‡
        token_count = 0
        for item in data:
            if isinstance(item, dict):
                content = item.get("content", "")
            else:
                content = item
            token_count += len(tokenizer.encode_string(content))

        estimated_usage = token_count * 50  # ä¼°ç®—ä½¿ç”¨é‡
        return token_count, estimated_usage
    
    def setup_batch_logging(self, log_file: str):
        """è®¾ç½®æ‰¹é‡å¤„ç†æ—¥å¿—"""
        self.batch_logger = logging.getLogger('batch_processing')
        self.batch_logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
        for handler in self.batch_logger.handlers[:]:
            self.batch_logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        
        self.batch_logger.addHandler(file_handler)
        
        # è®°å½•å¼€å§‹ä¿¡æ¯
        self.batch_logger.info("=" * 80)
        self.batch_logger.info("KGE-Gen æ‰¹é‡å¤„ç†å¼€å§‹")
        self.batch_logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.batch_logger.info("=" * 80)
    
    def log_batch_config(self, args):
        """è®°å½•æ‰¹é‡å¤„ç†é…ç½®"""
        if not self.batch_logger:
            return
            
        config_info = {
            "æ¨¡å‹é…ç½®": {
                "ä½¿ç”¨Traineeæ¨¡å‹": args.use_trainee_model,
                "Synthesizer URL": args.synthesizer_url,
                "Synthesizer æ¨¡å‹": args.synthesizer_model,
                "Trainee URL": args.trainee_url,
                "Trainee æ¨¡å‹": args.trainee_model,
            },
            "ç”Ÿæˆé…ç½®": {
                "æ–‡æœ¬å—å¤§å°": args.chunk_size,
                "æ–‡æœ¬å—é‡å ": args.chunk_overlap,
                "Tokenizer": args.tokenizer,
                "è¾“å‡ºæ•°æ®ç±»å‹": args.output_data_type,
                "è¾“å‡ºæ•°æ®æ ¼å¼": args.output_data_format,
                "æµ‹éªŒæ ·æœ¬æ•°": args.quiz_samples,
            },
            "éå†ç­–ç•¥": {
                "åŒå‘éå†": args.bidirectional,
                "æ‰©å±•æ–¹æ³•": args.expand_method,
                "æœ€å¤§é¢å¤–è¾¹æ•°": args.max_extra_edges,
                "æœ€å¤§tokenæ•°": args.max_tokens,
                "æœ€å¤§æ·±åº¦": args.max_depth,
                "è¾¹é‡‡æ ·ç­–ç•¥": args.edge_sampling,
                "å­¤ç«‹èŠ‚ç‚¹ç­–ç•¥": args.isolated_node_strategy,
                "æŸå¤±ç­–ç•¥": args.loss_strategy,
            },
            "é™åˆ¶é…ç½®": {
                "æ¯åˆ†é’Ÿè¯·æ±‚æ•°": args.rpm,
                "æ¯åˆ†é’Ÿtokenæ•°": args.tpm,
            }
        }
        
        self.batch_logger.info("æ‰¹é‡å¤„ç†é…ç½®:")
        for category, settings in config_info.items():
            self.batch_logger.info(f"  {category}:")
            for key, value in settings.items():
                self.batch_logger.info(f"    {key}: {value}")
    
    def log_file_result(self, file_path: str, success: bool, tokens_used: int, 
                       synthesizer_tokens: int, trainee_tokens: int, 
                       processing_time: float, output_file: str = None, error_msg: str = None):
        """è®°å½•å•ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœ"""
        if not self.batch_logger:
            return
            
        self.batch_logger.info("-" * 60)
        self.batch_logger.info(f"æ–‡ä»¶: {file_path}")
        self.batch_logger.info(f"å¤„ç†çŠ¶æ€: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        if success:
            self.batch_logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
            self.batch_logger.info(f"æ€»tokenä½¿ç”¨é‡: {tokens_used}")
            self.batch_logger.info(f"Synthesizer tokens: {synthesizer_tokens}")
            self.batch_logger.info(f"Trainee tokens: {trainee_tokens}")
            self.batch_logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        else:
            self.batch_logger.info(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.batch_stats["file_stats"].append({
            "file_path": file_path,
            "success": success,
            "tokens_used": tokens_used,
            "synthesizer_tokens": synthesizer_tokens,
            "trainee_tokens": trainee_tokens,
            "processing_time": processing_time,
            "output_file": output_file,
            "error_msg": error_msg
        })
        
        if success:
            self.batch_stats["processed_files"] += 1
            self.batch_stats["total_tokens"] += tokens_used
        else:
            self.batch_stats["failed_files"] += 1
    
    def log_batch_summary(self):
        """è®°å½•æ‰¹é‡å¤„ç†æ€»ç»“"""
        if not self.batch_logger:
            return
            
        self.batch_logger.info("=" * 80)
        self.batch_logger.info("æ‰¹é‡å¤„ç†æ€»ç»“")
        self.batch_logger.info(f"æ€»æ–‡ä»¶æ•°: {self.batch_stats['total_files']}")
        self.batch_logger.info(f"æˆåŠŸå¤„ç†: {self.batch_stats['processed_files']}")
        self.batch_logger.info(f"å¤„ç†å¤±è´¥: {self.batch_stats['failed_files']}")
        self.batch_logger.info(f"æ€»tokenä½¿ç”¨é‡: {self.batch_stats['total_tokens']}")
        self.batch_logger.info(f"æ€»å¤„ç†æ—¶é—´: {self.batch_stats['total_time']:.2f}ç§’")
        
        if self.batch_stats["processed_files"] > 0:
            avg_time = self.batch_stats["total_time"] / self.batch_stats["processed_files"]
            self.batch_logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’/æ–‡ä»¶")
        
        self.batch_logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.batch_logger.info("=" * 80)
    
    def process_single_file(self, file_path: str, args, progress_bar=None) -> dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        start_time = time.time()
        result = {
            "file_path": file_path,
            "success": False,
            "tokens_used": 0,
            "synthesizer_tokens": 0,
            "trainee_tokens": 0,
            "processing_time": 0,
            "output_file": None,
            "error_msg": None
        }
        
        try:
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            if progress_bar:
                progress_bar.set_description(f"å¤„ç† {os.path.basename(file_path)}")
            
            # æ„å»ºé…ç½®
            config = {
                "if_trainee_model": args.use_trainee_model,
                "read": {
                    "input_file": file_path,
                },
                "split": {
                    "chunk_size": args.chunk_size,
                    "chunk_overlap": args.chunk_overlap,
                    # æ‰¹é‡è¯·æ±‚ä¼˜åŒ–
                    "enable_batch_requests": True,
                    "batch_size": 30,
                    "max_wait_time": 1.0,
                    "use_adaptive_batching": True,
                    "min_batch_size": 10,
                    "max_batch_size": 50,
                    "enable_extraction_cache": True,
                    # Promptåˆå¹¶ä¼˜åŒ–
                    "enable_prompt_merging": True,
                    "prompt_merge_size": 5,
                },
                "output_data_type": args.output_data_type,
                "output_data_format": args.output_data_format,
                "tokenizer": args.tokenizer,
                "search": {"enabled": False},
                "quiz_and_judge": {
                    "enabled": args.use_trainee_model,
                    "quiz_samples": args.quiz_samples,
                    "re_judge": False,
                    # æ‰¹é‡è¯·æ±‚ä¼˜åŒ–
                    "enable_batch_requests": True,
                    "batch_size": 30,
                    "max_wait_time": 1.0,
                },
                "traverse_strategy": {
                    "bidirectional": args.bidirectional,
                    "expand_method": args.expand_method,
                    "max_extra_edges": args.max_extra_edges,
                    "max_tokens": args.max_tokens,
                    "max_depth": args.max_depth,
                    "edge_sampling": args.edge_sampling,
                    "isolated_node_strategy": args.isolated_node_strategy,
                    "loss_strategy": args.loss_strategy,
                },
            }

            env = {
                "SYNTHESIZER_BASE_URL": args.synthesizer_url,
                "SYNTHESIZER_MODEL": args.synthesizer_model,
                "TRAINEE_BASE_URL": args.trainee_url,
                "TRAINEE_MODEL": args.trainee_model,
                "SYNTHESIZER_API_KEY": args.api_key,
                "TRAINEE_API_KEY": args.trainee_api_key,
                "RPM": args.rpm,
                "TPM": args.tpm,
            }

            # åˆå§‹åŒ– KGE-Gen
            graph_gen = self.init_graph_gen(config, env)
            graph_gen.clear()

            # å¤„ç†æ•°æ®
            graph_gen.insert(
                read_config=config["read"],
                split_config=config["split"]
            )

            # æ„å»º quiz_and_judge é…ç½®
            quiz_and_judge_config = {
                "enabled": config["if_trainee_model"],
                "quiz_samples": config["quiz_and_judge"]["quiz_samples"],
                "re_judge": False,
                # æ‰¹é‡è¯·æ±‚ä¼˜åŒ–
                "enable_batch_requests": config["quiz_and_judge"].get("enable_batch_requests", True),
                "batch_size": config["quiz_and_judge"].get("batch_size", 30),
                "max_wait_time": config["quiz_and_judge"].get("max_wait_time", 1.0),
            }
            
            if quiz_and_judge_config["enabled"]:
                graph_gen.quiz_and_judge(quiz_and_judge_config=quiz_and_judge_config)

            # æ„å»º partition é…ç½®ï¼ˆä½¿ç”¨ ECE æ–¹æ³•ï¼‰
            traverse_strategy = config.get("traverse_strategy", {})
            edge_sampling = traverse_strategy.get("edge_sampling", "max_loss")
            
            # å¦‚æœ unit_sampling ä¸æ˜¯ "random" ä½†æ²¡æœ‰ä½¿ç”¨ trainee æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ "random"
            # å› ä¸º min_loss å’Œ max_loss éœ€è¦ quiz_and_judge æ¥è·å– loss å€¼
            if edge_sampling in ["min_loss", "max_loss"] and not config["if_trainee_model"]:
                edge_sampling = "random"
            
            partition_config = {
                "method": "ece",
                "method_params": {
                    "max_units_per_community": traverse_strategy.get("max_extra_edges", 5) + 10,
                    "min_units_per_community": 3,
                    "max_tokens_per_community": traverse_strategy.get("max_tokens", 256) * 40,
                    "unit_sampling": edge_sampling,
                }
            }

            # æ„å»º generate é…ç½®
            generate_config = {
                "mode": config["output_data_type"],  # æ”¯æŒ "atomic", "multi_hop", "aggregated", "cot", "all"
                "data_format": config["output_data_format"],
                # ä¼˜åŒ–é…ç½®
                "use_multi_template": True,
                "template_seed": None,
                "chinese_only": getattr(args, "chinese_only", False),
                # æ‰¹é‡è¯·æ±‚é…ç½®
                "enable_batch_requests": True,
                "batch_size": 30,
                "max_wait_time": 1.0,
                "use_adaptive_batching": True,
                "min_batch_size": 10,
                "max_batch_size": 50,
                # ç¼“å­˜ä¼˜åŒ–
                "enable_prompt_cache": True,
                "cache_max_size": 50000,
                "cache_ttl": None,
                # åˆå¹¶æ¨¡å¼ä¼˜åŒ–
                "use_combined_mode": True,
                # å»é‡ä¼˜åŒ–
                "enable_deduplication": True,
                "persistent_deduplication": True,
                # ç”Ÿæˆæ•°é‡ä¸æ¯”ä¾‹
                "target_qa_pairs": getattr(args, "qa_pair_limit", None),
                "mode_ratios": {
                    "atomic": 25.0,
                    "aggregated": 25.0,
                    "multi_hop": 25.0,
                    "cot": 25.0,
                },
            }

            # ç”Ÿæˆé—®ç­”å¯¹
            graph_gen.generate(
                partition_config=partition_config,
                generate_config=generate_config
            )

            # ä¿å­˜è¾“å‡º
            output_data = graph_gen.qa_storage.data
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            output_file = f"{base_name}_graphgen_output.jsonl"
            
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            # è®¡ç®—å®é™…ä½¿ç”¨çš„ token
            synthesizer_tokens = sum(u["total_tokens"] for u in graph_gen.synthesizer_llm_client.token_usage)
            trainee_tokens = sum(u["total_tokens"] for u in graph_gen.trainee_llm_client.token_usage) if config["if_trainee_model"] else 0
            total_tokens = synthesizer_tokens + trainee_tokens

            processing_time = time.time() - start_time
            
            result.update({
                "success": True,
                "tokens_used": total_tokens,
                "synthesizer_tokens": synthesizer_tokens,
                "trainee_tokens": trainee_tokens,
                "processing_time": processing_time,
                "output_file": output_file
            })

            # æ¸…ç†å·¥ä½œç©ºé—´
            cleanup_workspace(graph_gen.working_dir)

        except Exception as e:
            processing_time = time.time() - start_time
            result.update({
                "processing_time": processing_time,
                "error_msg": str(e)
            })
            
            if progress_bar:
                progress_bar.set_description(f"å¤±è´¥: {os.path.basename(file_path)}")

        return result
    
    def run_batch_processing(self, args):
        """è¿è¡Œæ‰¹é‡å¤„ç†"""
        batch_start_time = time.time()
        
        # è®¾ç½®æ‰¹é‡å¤„ç†æ—¥å¿—
        log_file = f"graphgen_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        self.setup_batch_logging(log_file)
        
        # è®°å½•é…ç½®
        self.log_batch_config(args)
        
        # éªŒè¯æ‰€æœ‰æ–‡ä»¶
        valid_files = []
        for file_path in args.input_files:
            if not os.path.exists(file_path):
                print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
                continue
            
            valid_extensions = ['.txt', '.json', '.jsonl', '.csv']
            if not any(file_path.endswith(ext) for ext in valid_extensions):
                print(f"âš ï¸  è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                continue
                
            valid_files.append(file_path)
        
        if not valid_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è¿›è¡Œå¤„ç†")
            return False
        
        self.batch_stats["total_files"] = len(valid_files)
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(valid_files)} ä¸ªæ–‡ä»¶...")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        
        # æµ‹è¯• API è¿æ¥
        print("ğŸ”— æµ‹è¯• API è¿æ¥...")
        if not self.test_api_connection(args.synthesizer_url, args.api_key, args.synthesizer_model):
            return False
            
        if args.use_trainee_model:
            if not self.test_api_connection(args.trainee_url, args.trainee_api_key, args.trainee_model):
                return False
        
        # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ–‡ä»¶
        with tqdm(total=len(valid_files), desc="æ‰¹é‡å¤„ç†", unit="æ–‡ä»¶") as pbar:
            for file_path in valid_files:
                result = self.process_single_file(file_path, args, pbar)
                
                # è®°å½•ç»“æœ
                self.log_file_result(
                    result["file_path"], 
                    result["success"],
                    result["tokens_used"],
                    result["synthesizer_tokens"],
                    result["trainee_tokens"],
                    result["processing_time"],
                    result["output_file"],
                    result["error_msg"]
                )
                
                pbar.update(1)
        
        # è®°å½•æ€»ç»“
        self.batch_stats["total_time"] = time.time() - batch_start_time
        self.log_batch_summary()
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š æ‰¹é‡å¤„ç†æ€»ç»“:")
        print(f"   æ€»æ–‡ä»¶æ•°: {self.batch_stats['total_files']}")
        print(f"   æˆåŠŸå¤„ç†: {self.batch_stats['processed_files']}")
        print(f"   å¤„ç†å¤±è´¥: {self.batch_stats['failed_files']}")
        print(f"   æ€»tokenä½¿ç”¨é‡: {self.batch_stats['total_tokens']}")
        print(f"   æ€»å¤„ç†æ—¶é—´: {self.batch_stats['total_time']:.2f}ç§’")
        print(f"   æ—¥å¿—æ–‡ä»¶: {log_file}")
        print("=" * 60)
        
        return self.batch_stats["failed_files"] == 0
    
    def run_graphgen(self, args):
        """è¿è¡Œ KGE-Gen ä¸»æµç¨‹"""
        print("ğŸš€ å¼€å§‹è¿è¡Œ KGE-Gen...")
        
        # æ„å»ºé…ç½®
        config = {
            "if_trainee_model": args.use_trainee_model,
            "read": {
                "input_file": args.input_file,
            },
            "split": {
                "chunk_size": args.chunk_size,
                "chunk_overlap": args.chunk_overlap,
            },
            "output_data_type": args.output_data_type,
            "output_data_format": args.output_data_format,
            "tokenizer": args.tokenizer,
            "search": {"enabled": False},
            "quiz_and_judge_strategy": {
                "enabled": args.use_trainee_model,
                "quiz_samples": args.quiz_samples,
            },
            "traverse_strategy": {
                "bidirectional": args.bidirectional,
                "expand_method": args.expand_method,
                "max_extra_edges": args.max_extra_edges,
                "max_tokens": args.max_tokens,
                "max_depth": args.max_depth,
                "edge_sampling": args.edge_sampling,
                "isolated_node_strategy": args.isolated_node_strategy,
                "loss_strategy": args.loss_strategy,
            },
        }

        env = {
            "SYNTHESIZER_BASE_URL": args.synthesizer_url,
            "SYNTHESIZER_MODEL": args.synthesizer_model,
            "TRAINEE_BASE_URL": args.trainee_url,
            "TRAINEE_MODEL": args.trainee_model,
            "SYNTHESIZER_API_KEY": args.api_key,
            "TRAINEE_API_KEY": args.trainee_api_key,
            "RPM": args.rpm,
            "TPM": args.tpm,
        }
        graph_gen = self.init_graph_gen(config, env)

        # æµ‹è¯• API è¿æ¥
        print("ğŸ”— æµ‹è¯• API è¿æ¥...")
        if not self.test_api_connection(env["SYNTHESIZER_BASE_URL"], env["SYNTHESIZER_API_KEY"], env["SYNTHESIZER_MODEL"]):
            return False
            
        if config["if_trainee_model"]:
            if not self.test_api_connection(env["TRAINEE_BASE_URL"], env["TRAINEE_API_KEY"], env["TRAINEE_MODEL"]):
                return False

        # è®¡ç®— token ä½¿ç”¨é‡
        print("ğŸ“Š è®¡ç®— token ä½¿ç”¨é‡...")
        token_count, estimated_usage = self.count_tokens(args.input_file, args.tokenizer)
        print(f"ğŸ“ æºæ–‡æœ¬ token æ•°é‡: {token_count}")
        print(f"ğŸ“ˆ é¢„è®¡ token ä½¿ç”¨é‡: {estimated_usage}")

        # åˆå§‹åŒ– KGE-Gen
        print("ğŸ”§ åˆå§‹åŒ– KGE-Gen...")
       
        graph_gen.clear()

        try:
            # å¤„ç†æ•°æ®
            print("ğŸ“– å¤„ç†è¾“å…¥æ•°æ®...")
            graph_gen.insert(
                read_config=config["read"],
                split_config=config["split"]
            )

            # æ„å»º quiz_and_judge é…ç½®
            quiz_and_judge_config = {
                "enabled": config["if_trainee_model"],
                "quiz_samples": config["quiz_and_judge_strategy"]["quiz_samples"],
                "re_judge": False,
            }
            
            if quiz_and_judge_config["enabled"]:
                # ç”Ÿæˆæµ‹éªŒå’Œåˆ¤æ–­
                print("â“ ç”Ÿæˆæµ‹éªŒå’Œåˆ¤æ–­...")
                graph_gen.quiz_and_judge(quiz_and_judge_config=quiz_and_judge_config)

            # æ„å»º partition é…ç½®ï¼ˆä½¿ç”¨ ECE æ–¹æ³•ï¼‰
            traverse_strategy = config.get("traverse_strategy", {})
            edge_sampling = traverse_strategy.get("edge_sampling", "max_loss")
            
            # å¦‚æœ unit_sampling ä¸æ˜¯ "random" ä½†æ²¡æœ‰ä½¿ç”¨ trainee æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ "random"
            # å› ä¸º min_loss å’Œ max_loss éœ€è¦ quiz_and_judge æ¥è·å– loss å€¼
            if edge_sampling in ["min_loss", "max_loss"] and not config["if_trainee_model"]:
                edge_sampling = "random"
            
            partition_config = {
                "method": "ece",
                "method_params": {
                    "max_units_per_community": traverse_strategy.get("max_extra_edges", 5) + 10,
                    "min_units_per_community": 3,
                    "max_tokens_per_community": traverse_strategy.get("max_tokens", 256) * 40,
                    "unit_sampling": edge_sampling,
                }
            }

            # æ„å»º generate é…ç½®
            generate_config = {
                "mode": config["output_data_type"],  # æ”¯æŒ "atomic", "multi_hop", "aggregated", "all"
                "data_format": config["output_data_format"],
            }

            # ç”Ÿæˆé—®ç­”å¯¹
            print("ğŸ”„ ç”Ÿæˆé—®ç­”å¯¹...")
            graph_gen.generate(
                partition_config=partition_config,
                generate_config=generate_config
            )

            # ä¿å­˜è¾“å‡º
            print("ğŸ’¾ ä¿å­˜è¾“å‡º...")
            output_data = graph_gen.qa_storage.data
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
            if args.output_file:
                output_file = args.output_file
            else:
                base_name = os.path.splitext(os.path.basename(args.input_file))[0]
                output_file = f"{base_name}_graphgen_output.jsonl"
            
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            # è®¡ç®—å®é™…ä½¿ç”¨çš„ token
            synthesizer_tokens = sum(u["total_tokens"] for u in graph_gen.synthesizer_llm_client.token_usage)
            trainee_tokens = sum(u["total_tokens"] for u in graph_gen.trainee_llm_client.token_usage) if config["if_trainee_model"] else 0
            total_tokens = synthesizer_tokens + trainee_tokens

            print("âœ… KGE-Gen è¿è¡Œå®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            print(f"ğŸ”¢ å®é™…ä½¿ç”¨ token: {total_tokens}")
            print(f"ğŸ“Š Synthesizer tokens: {synthesizer_tokens}")
            if config["if_trainee_model"]:
                print(f"ğŸ“Š Trainee tokens: {trainee_tokens}")

            return True

        except Exception as e:
            print(f"âŒ è¿è¡Œå‡ºé”™: {str(e)}")
            return False

        finally:
            # æ¸…ç†å·¥ä½œç©ºé—´
            cleanup_workspace(graph_gen.working_dir)


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="KGE-Gen å‘½ä»¤è¡Œå·¥å…· - ä»æ–‡æœ¬ç”ŸæˆçŸ¥è¯†å›¾è°±è®­ç»ƒæ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å•ä¸ªæ–‡ä»¶å¤„ç†
  python graphgen_cli.py -i input.txt -k your_api_key

  # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
  python graphgen_cli.py -b file1.txt file2.json file3.csv -k your_api_key

  # ä»æ–‡ä»¶åˆ—è¡¨æ‰¹é‡å¤„ç†
  python graphgen_cli.py -l file_list.txt -k your_api_key

  # ä½¿ç”¨ Trainee æ¨¡å‹
  python graphgen_cli.py -i input.txt -k your_api_key --use-trainee-model --trainee-api-key your_trainee_key

  # è‡ªå®šä¹‰å‚æ•°
  python graphgen_cli.py -i input.txt -k your_api_key --chunk-size 2048 --max-depth 3
        """
    )

    # è¾“å…¥å‚æ•°ç»„ - äº’æ–¥é€‰æ‹©
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("-i", "--input-file", help="å•ä¸ªè¾“å…¥æ–‡ä»¶è·¯å¾„ (.txt, .json, .jsonl, .csv)")
    input_group.add_argument("-b", "--batch-files", nargs='+', help="æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶è·¯å¾„")
    input_group.add_argument("-l", "--file-list", help="åŒ…å«æ–‡ä»¶è·¯å¾„åˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶")
    
    # å…¶ä»–å¿…éœ€å‚æ•°
    parser.add_argument("-k", "--api-key", 
                      default=os.getenv("SYNTHESIZER_API_KEY", "sk-wFHN2ySjUYxCx3LrWAkJEMB11FMxYDvF6DHdye9yVDwIH2no"), 
                      help="SiliconFlow API Key (é»˜è®¤ä»ç¯å¢ƒå˜é‡ SYNTHESIZER_API_KEY è¯»å–)")
    parser.add_argument("-o", "--output-file", help="å•ä¸ªæ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ (æ‰¹é‡å¤„ç†æ—¶å¿½ç•¥)")

    # æ¨¡å‹é…ç½®
    model_group = parser.add_argument_group("æ¨¡å‹é…ç½®")
    model_group.add_argument("--use-trainee-model", action="store_true", help="ä½¿ç”¨ Trainee æ¨¡å‹")
    model_group.add_argument("--synthesizer-url", 
                           default=os.getenv("SYNTHESIZER_BASE_URL", "https://api.huiyan-ai.cn/v1"), 
                           help="Synthesizer API URL (é»˜è®¤ä»ç¯å¢ƒå˜é‡ SYNTHESIZER_BASE_URL è¯»å–)")
    model_group.add_argument("--synthesizer-model", 
                           default=os.getenv("SYNTHESIZER_MODEL", "gpt-4.1-mini-2025-04-14"), 
                           help="Synthesizer æ¨¡å‹åç§° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ SYNTHESIZER_MODEL è¯»å–)")
    model_group.add_argument("--trainee-url", 
                           default=os.getenv("TRAINEE_BASE_URL", "https://api.siliconflow.cn/v1"), 
                           help="Trainee API URL (é»˜è®¤ä»ç¯å¢ƒå˜é‡ TRAINEE_BASE_URL è¯»å–)")
    model_group.add_argument("--trainee-model", 
                           default=os.getenv("TRAINEE_MODEL", "Qwen/Qwen2.5-7B-Instruct"), 
                           help="Trainee æ¨¡å‹åç§° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ TRAINEE_MODEL è¯»å–)")
    model_group.add_argument("--trainee-api-key", 
                           default=os.getenv("TRAINEE_API_KEY", ""), 
                           help="Trainee æ¨¡å‹çš„ API Key (é»˜è®¤ä»ç¯å¢ƒå˜é‡ TRAINEE_API_KEY è¯»å–)")

    # ç”Ÿæˆé…ç½®
    gen_group = parser.add_argument_group("ç”Ÿæˆé…ç½®")
    gen_group.add_argument("--chunk-size", type=int, 
                          default=int(os.getenv("CHUNK_SIZE", "1024")), 
                          help="æ–‡æœ¬å—å¤§å° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ CHUNK_SIZE è¯»å–)")
    gen_group.add_argument("--chunk-overlap", type=int, 
                          default=int(os.getenv("CHUNK_OVERLAP", "100")), 
                          help="æ–‡æœ¬å—é‡å å¤§å° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ CHUNK_OVERLAP è¯»å–)")
    gen_group.add_argument("--tokenizer", 
                          default=os.getenv("TOKENIZER", "cl100k_base"), 
                          help="Tokenizer åç§° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ TOKENIZER è¯»å–)")
    gen_group.add_argument("--output-data-type", choices=["atomic", "multi_hop", "aggregated", "cot", "all"], 
                          default=os.getenv("OUTPUT_DATA_TYPE", "aggregated"), 
                          help="è¾“å‡ºæ•°æ®ç±»å‹ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ OUTPUT_DATA_TYPE è¯»å–)")
    gen_group.add_argument("--output-data-format", choices=["Alpaca", "Sharegpt", "ChatML"], 
                          default=os.getenv("OUTPUT_DATA_FORMAT", "Alpaca"), 
                          help="è¾“å‡ºæ•°æ®æ ¼å¼ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ OUTPUT_DATA_FORMAT è¯»å–)")
    gen_group.add_argument("--quiz-samples", type=int, 
                          default=int(os.getenv("QUIZ_SAMPLES", "2")), 
                          help="æµ‹éªŒæ ·æœ¬æ•°é‡ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ QUIZ_SAMPLES è¯»å–)")
    gen_group.add_argument("--chinese-only", action="store_true",
                          default=os.getenv("CHINESE_ONLY", "false").lower() == "true",
                          help="åªç”Ÿæˆä¸­æ–‡é—®ç­”å¯¹ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ CHINESE_ONLY è¯»å–)")
    gen_group.add_argument("--qa-pair-limit", type=int,
                          default=int(os.getenv("QA_PAIR_LIMIT", "0")) or None,
                          help="ç›®æ ‡QAå¯¹æ•°é‡é™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ QA_PAIR_LIMIT è¯»å–)")

    # éå†ç­–ç•¥
    traverse_group = parser.add_argument_group("éå†ç­–ç•¥")
    traverse_group.add_argument("--bidirectional", action="store_true", 
                               default=os.getenv("BIDIRECTIONAL", "True").lower() == "true", 
                               help="åŒå‘éå† (é»˜è®¤ä»ç¯å¢ƒå˜é‡ BIDIRECTIONAL è¯»å–)")
    traverse_group.add_argument("--expand-method", choices=["max_width", "max_tokens"], 
                               default=os.getenv("EXPAND_METHOD", "max_tokens"), 
                               help="æ‰©å±•æ–¹æ³• (é»˜è®¤ä»ç¯å¢ƒå˜é‡ EXPAND_METHOD è¯»å–)")
    traverse_group.add_argument("--max-extra-edges", type=int, 
                               default=int(os.getenv("MAX_EXTRA_EDGES", "5")), 
                               help="æœ€å¤§é¢å¤–è¾¹æ•° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ MAX_EXTRA_EDGES è¯»å–)")
    traverse_group.add_argument("--max-tokens", type=int, 
                               default=int(os.getenv("MAX_TOKENS", "256")), 
                               help="æœ€å¤§ token æ•° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ MAX_TOKENS è¯»å–)")
    traverse_group.add_argument("--max-depth", type=int, 
                               default=int(os.getenv("MAX_DEPTH", "2")), 
                               help="æœ€å¤§æ·±åº¦ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ MAX_DEPTH è¯»å–)")
    traverse_group.add_argument("--edge-sampling", choices=["max_loss", "min_loss", "random"], 
                               default=os.getenv("EDGE_SAMPLING", "max_loss"), 
                               help="è¾¹é‡‡æ ·ç­–ç•¥ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ EDGE_SAMPLING è¯»å–)")
    traverse_group.add_argument("--isolated-node-strategy", choices=["add", "ignore"], 
                               default=os.getenv("ISOLATED_NODE_STRATEGY", "ignore"), 
                               help="å­¤ç«‹èŠ‚ç‚¹ç­–ç•¥ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ ISOLATED_NODE_STRATEGY è¯»å–)")
    traverse_group.add_argument("--loss-strategy", choices=["only_edge", "both"], 
                               default=os.getenv("LOSS_STRATEGY", "only_edge"), 
                               help="æŸå¤±ç­–ç•¥ (é»˜è®¤ä»ç¯å¢ƒå˜é‡ LOSS_STRATEGY è¯»å–)")

    # é™åˆ¶é…ç½®
    limit_group = parser.add_argument_group("é™åˆ¶é…ç½®")
    limit_group.add_argument("--rpm", type=int, 
                           default=int(os.getenv("RPM", "1000")), 
                           help="æ¯åˆ†é’Ÿè¯·æ±‚æ•° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ RPM è¯»å–)")
    limit_group.add_argument("--tpm", type=int, 
                           default=int(os.getenv("TPM", "50000")), 
                           help="æ¯åˆ†é’Ÿ token æ•° (é»˜è®¤ä»ç¯å¢ƒå˜é‡ TPM è¯»å–)")

    # æµ‹è¯•è¿æ¥
    parser.add_argument("--test-connection", action="store_true", help="ä»…æµ‹è¯• API è¿æ¥")

    return parser


def load_file_list(file_list_path: str) -> list:
    """ä»æ–‡ä»¶ä¸­åŠ è½½æ–‡ä»¶è·¯å¾„åˆ—è¡¨"""
    try:
        with open(file_list_path, 'r', encoding='utf-8') as f:
            files = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        return files
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶åˆ—è¡¨ {file_list_path}: {str(e)}")
        sys.exit(1)

def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # éªŒè¯ API Key
    if not args.api_key:
        print("âŒ é”™è¯¯: æœªæä¾› API Key")
        print("è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æä¾› API Key:")
        print("1. å‘½ä»¤è¡Œå‚æ•°: -k your_api_key")
        print("2. ç¯å¢ƒå˜é‡: SYNTHESIZER_API_KEY=your_api_key")
        print("3. .env æ–‡ä»¶: SYNTHESIZER_API_KEY=your_api_key")
        sys.exit(1)

    # ç¡®å®šè¾“å…¥æ–‡ä»¶åˆ—è¡¨
    input_files = []
    
    if args.input_file:
        # å•ä¸ªæ–‡ä»¶å¤„ç†
        input_files = [args.input_file]
    elif args.batch_files:
        # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        input_files = args.batch_files
    elif args.file_list:
        # ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½
        input_files = load_file_list(args.file_list)
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    valid_extensions = ['.txt', '.json', '.jsonl', '.csv']
    invalid_files = []
    
    for file_path in input_files:
        if not os.path.exists(file_path):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            invalid_files.append(file_path)
        elif not any(file_path.endswith(ext) for ext in valid_extensions):
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
            invalid_files.append(file_path)
    
    if invalid_files:
        print(f"âŒ å‘ç° {len(invalid_files)} ä¸ªæ— æ•ˆæ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)

    # å¦‚æœä½¿ç”¨ trainee æ¨¡å‹ä½†æ²¡æœ‰æä¾› trainee api keyï¼Œä½¿ç”¨ä¸» api key
    if args.use_trainee_model and not args.trainee_api_key:
        args.trainee_api_key = args.api_key

    cli = GraphGenCLI()

    # å¦‚æœåªæ˜¯æµ‹è¯•è¿æ¥
    if args.test_connection:
        print("ğŸ”— æµ‹è¯• API è¿æ¥...")
        success = cli.test_api_connection(args.synthesizer_url, args.api_key, args.synthesizer_model)
        if args.use_trainee_model:
            success &= cli.test_api_connection(args.trainee_url, args.trainee_api_key, args.trainee_model)
        
        if success:
            print("âœ… æ‰€æœ‰ API è¿æ¥æµ‹è¯•é€šè¿‡")
            sys.exit(0)
        else:
            print("âŒ API è¿æ¥æµ‹è¯•å¤±è´¥")
            sys.exit(1)

    # åˆ¤æ–­æ˜¯å•ä¸ªæ–‡ä»¶è¿˜æ˜¯æ‰¹é‡å¤„ç†
    if len(input_files) == 1:
        # å•ä¸ªæ–‡ä»¶å¤„ç†
        args.input_file = input_files[0]
        success = cli.run_graphgen(args)
    else:
        # æ‰¹é‡å¤„ç†
        args.input_files = input_files
        success = cli.run_batch_processing(args)
    
    if success:
        print("ğŸ‰ ä»»åŠ¡å®Œæˆ!")
        sys.exit(0)
    else:
        print("ğŸ’¥ ä»»åŠ¡å¤±è´¥!")
        sys.exit(1)


if __name__ == "__main__":
    main()
