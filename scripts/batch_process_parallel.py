#!/usr/bin/env python3
"""
GraphGen å¹¶è¡Œæ‰¹é‡å¤„ç†è„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
æ”¯æŒå¤šæ¨¡å‹å¹¶è¡Œå¤„ç†å’Œ batch size é…ç½®
"""

import os
import sys
import json
import time
import logging
import threading
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty
from tqdm import tqdm
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from batch_process import BatchProcessor


class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    
    def __init__(
        self,
        api_key: str,
        synthesizer_url: str,
        synthesizer_model: str,
        trainee_url: Optional[str] = None,
        trainee_model: Optional[str] = None,
        trainee_api_key: Optional[str] = None,
        model_id: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹é…ç½®
        
        Args:
            api_key: API Key
            synthesizer_url: Synthesizer API URL
            synthesizer_model: Synthesizer æ¨¡å‹åç§°
            trainee_url: Trainee API URL
            trainee_model: Trainee æ¨¡å‹åç§°
            trainee_api_key: Trainee API Key
            model_id: æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        self.api_key = api_key
        self.synthesizer_url = synthesizer_url
        self.synthesizer_model = synthesizer_model
        self.trainee_url = trainee_url or synthesizer_url
        self.trainee_model = trainee_model or synthesizer_model
        self.trainee_api_key = trainee_api_key or api_key
        self.model_id = model_id or f"{synthesizer_url}_{synthesizer_model}"
        self.current_tasks = 0  # å½“å‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡æ•°
        self.total_tasks = 0  # æ€»ä»»åŠ¡æ•°
        self.completed_tasks = 0  # å·²å®Œæˆä»»åŠ¡æ•°


class ParallelBatchProcessor:
    """å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨ç±» - æ”¯æŒå¤šæ¨¡å‹å¹¶è¡Œå¤„ç†"""
    
    def __init__(
        self,
        model_configs: List[Union[Dict, ModelConfig]],
        use_trainee_model: bool = False,
        output_dir: str = "tasks/outputs",
        log_dir: str = "logs",
        batch_size: int = 1,
        max_workers: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
        
        Args:
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªé…ç½®å¯ä»¥æ˜¯å­—å…¸æˆ– ModelConfig å¯¹è±¡
                [
                    {
                        "api_key": "key1",
                        "synthesizer_url": "url1",
                        "synthesizer_model": "model1",
                        ...
                    },
                    {
                        "api_key": "key2",
                        "synthesizer_url": "url2",
                        "synthesizer_model": "model2",
                        ...
                    }
                ]
            use_trainee_model: æ˜¯å¦ä½¿ç”¨ Trainee æ¨¡å‹
            output_dir: è¾“å‡ºç›®å½•
            log_dir: æ—¥å¿—ç›®å½•
            batch_size: æ¯ä¸ªæ¨¡å‹åŒæ—¶å¤„ç†çš„ä»»åŠ¡æ•°ï¼ˆbatch sizeï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼šæ¨¡å‹æ•°é‡ * batch_sizeï¼‰
            **kwargs: å…¶ä»–é…ç½®å‚æ•°ï¼ˆä¼ é€’ç»™ BatchProcessorï¼‰
        """
        # è½¬æ¢æ¨¡å‹é…ç½®
        self.model_configs = []
        for config in model_configs:
            if isinstance(config, dict):
                self.model_configs.append(ModelConfig(**config))
            else:
                self.model_configs.append(config)
        
        if not self.model_configs:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªæ¨¡å‹é…ç½®")
        
        self.use_trainee_model = use_trainee_model
        self.output_dir = Path(output_dir)
        self.log_dir = Path(log_dir)
        self.batch_size = batch_size
        self.max_workers = max_workers or (len(self.model_configs) * batch_size)
        
        # åˆ›å»ºè¾“å‡ºå’Œæ—¥å¿—ç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å…¶ä»–é…ç½®å‚æ•°
        self.processor_kwargs = kwargs
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "total_tokens": 0,
            "total_time": 0,
            "file_results": [],
            "model_stats": {config.model_id: {
                "processed": 0,
                "failed": 0,
                "tokens": 0
            } for config in self.model_configs}
        }
        
        # ä»»åŠ¡é˜Ÿåˆ—å’Œé”
        self.task_queue = Queue()
        self.stats_lock = threading.Lock()
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.log_dir / f"parallel_batch_process_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - [%(threadName)s] - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"å¹¶è¡Œæ‰¹é‡å¤„ç†æ—¥å¿—æ–‡ä»¶: {log_file}")
        self.logger.info(f"é…ç½®äº† {len(self.model_configs)} ä¸ªæ¨¡å‹")
        self.logger.info(f"Batch size: {self.batch_size}, Max workers: {self.max_workers}")
    
    def _get_next_model(self) -> ModelConfig:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰"""
        # é€‰æ‹©å½“å‰ä»»åŠ¡æ•°æœ€å°‘çš„æ¨¡å‹
        return min(self.model_configs, key=lambda m: m.current_tasks)
    
    def _process_file_with_model(
        self,
        file_path: Union[str, Path],
        model_config: ModelConfig,
        output_file: Optional[Union[str, Path]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            model_config: æ¨¡å‹é…ç½®
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        model_config.current_tasks += 1
        model_config.total_tasks += 1
        
        try:
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
            if output_file is None:
                base_name = Path(file_path).stem
                model_suffix = model_config.model_id.replace("/", "_").replace(":", "_")
                output_file = self.output_dir / f"{base_name}_{model_suffix}_output.jsonl"
            else:
                output_file = Path(output_file)
            
            # åˆ›å»º BatchProcessor å®ä¾‹
            processor = BatchProcessor(
                api_key=model_config.api_key,
                synthesizer_url=model_config.synthesizer_url,
                synthesizer_model=model_config.synthesizer_model,
                trainee_url=model_config.trainee_url,
                trainee_model=model_config.trainee_model,
                trainee_api_key=model_config.trainee_api_key,
                use_trainee_model=self.use_trainee_model,
                output_dir=str(self.output_dir),
                log_dir=str(self.log_dir),
                **self.processor_kwargs
            )
            
            # å¤„ç†æ–‡ä»¶
            self.logger.info(f"[{model_config.model_id}] å¼€å§‹å¤„ç†: {file_path}")
            result = processor.process_file(file_path, output_file)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            with self.stats_lock:
                if result["success"]:
                    self.stats["processed_files"] += 1
                    self.stats["total_tokens"] += result.get("tokens_used", 0)
                    self.stats["model_stats"][model_config.model_id]["processed"] += 1
                    self.stats["model_stats"][model_config.model_id]["tokens"] += result.get("tokens_used", 0)
                    model_config.completed_tasks += 1
                else:
                    self.stats["failed_files"] += 1
                    self.stats["model_stats"][model_config.model_id]["failed"] += 1
                    model_config.completed_tasks += 1
                
                self.stats["file_results"].append({
                    **result,
                    "model_id": model_config.model_id
                })
            
            if result["success"]:
                self.logger.info(
                    f"[{model_config.model_id}] âœ… å¤„ç†æˆåŠŸ: {file_path} "
                    f"(tokens: {result.get('tokens_used', 0)})"
                )
            else:
                self.logger.error(
                    f"[{model_config.model_id}] âŒ å¤„ç†å¤±è´¥: {file_path} - "
                    f"{result.get('error_msg', 'æœªçŸ¥é”™è¯¯')}"
                )
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"[{model_config.model_id}] å¤„ç†å¼‚å¸¸: {file_path} - {error_msg}")
            
            with self.stats_lock:
                self.stats["failed_files"] += 1
                self.stats["model_stats"][model_config.model_id]["failed"] += 1
                model_config.completed_tasks += 1
                self.stats["file_results"].append({
                    "file_path": str(file_path),
                    "success": False,
                    "error_msg": error_msg,
                    "model_id": model_config.model_id
                })
            
            return {
                "file_path": str(file_path),
                "success": False,
                "error_msg": error_msg,
                "model_id": model_config.model_id
            }
        finally:
            model_config.current_tasks -= 1
    
    def _worker_thread(
        self,
        worker_id: int,
        progress_bar: Optional[tqdm] = None
    ):
        """
        å·¥ä½œçº¿ç¨‹å‡½æ•°
        
        Args:
            worker_id: å·¥ä½œçº¿ç¨‹ ID
            progress_bar: è¿›åº¦æ¡å¯¹è±¡
        """
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶1ç§’ï¼Œé¿å…æ— é™é˜»å¡ï¼‰
                task = self.task_queue.get(timeout=1)
                if task is None:  # ç»“æŸä¿¡å·
                    break
                
                file_path, output_file = task
                
                # é€‰æ‹©æ¨¡å‹ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
                model_config = self._get_next_model()
                
                # å¤„ç†æ–‡ä»¶
                result = self._process_file_with_model(file_path, model_config, output_file)
                
                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    progress_bar.update(1)
                    if result["success"]:
                        progress_bar.set_postfix({
                            "æˆåŠŸ": self.stats["processed_files"],
                            "å¤±è´¥": self.stats["failed_files"]
                        })
                
                self.task_queue.task_done()
                
            except Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…ï¼ˆè¿™æ˜¯æ­£å¸¸çš„è¶…æ—¶æƒ…å†µï¼‰
                continue
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸ï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­
                self.logger.error(f"å·¥ä½œçº¿ç¨‹ {worker_id} é”™è¯¯: {e}", exc_info=True)
                # å¦‚æœå·²ç»è·å–äº†ä»»åŠ¡ï¼Œéœ€è¦æ ‡è®°ä¸ºå®Œæˆ
                try:
                    self.task_queue.task_done()
                except ValueError:
                    # å¦‚æœ task_done() è¢«è°ƒç”¨æ¬¡æ•°è¶…è¿‡ put() æ¬¡æ•°ï¼Œå¿½ç•¥é”™è¯¯
                    pass
    
    def process_batch(
        self,
        file_paths: List[Union[str, Path]],
        output_dir: Optional[Union[str, Path]] = None,
        continue_on_error: bool = True
    ) -> Dict[str, Any]:
        """
        å¹¶è¡Œæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            continue_on_error: é‡åˆ°é”™è¯¯æ—¶æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœå­—å…¸
        """
        if output_dir:
            self.output_dir = Path(output_dir)
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        batch_start_time = time.time()
        self.stats = {
            "total_files": len(file_paths),
            "processed_files": 0,
            "failed_files": 0,
            "total_tokens": 0,
            "total_time": 0,
            "file_results": [],
            "model_stats": {config.model_id: {
                "processed": 0,
                "failed": 0,
                "tokens": 0
            } for config in self.model_configs}
        }
        
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç†")
        self.logger.info(f"æ–‡ä»¶æ•°é‡: {len(file_paths)}")
        self.logger.info(f"æ¨¡å‹æ•°é‡: {len(self.model_configs)}")
        self.logger.info(f"Batch size: {self.batch_size}")
        self.logger.info(f"æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info("=" * 80)
        
        # éªŒè¯æ–‡ä»¶
        valid_files = []
        for file_path in file_paths:
            file_path = Path(file_path)
            if not file_path.exists():
                self.logger.warning(f"è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
                continue
            
            valid_extensions = ['.txt', '.json', '.jsonl', '.csv']
            if not any(file_path.suffix.lower() == ext for ext in valid_extensions):
                self.logger.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                continue
            
            valid_files.append(file_path)
        
        if not valid_files:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è¿›è¡Œå¤„ç†")
            return {
                "success": False,
                "error": "æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶",
                "stats": self.stats
            }
        
        self.stats["total_files"] = len(valid_files)
        
        # å°†æ‰€æœ‰ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        for file_path in valid_files:
            self.task_queue.put((file_path, None))
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(total=len(valid_files), desc="å¹¶è¡Œæ‰¹é‡å¤„ç†", unit="æ–‡ä»¶")
        
        # åˆ›å»ºå·¥ä½œçº¿ç¨‹
        threads = []
        for i in range(self.max_workers):
            thread = threading.Thread(
                target=self._worker_thread,
                args=(i, pbar),
                daemon=True,
                name=f"Worker-{i}"
            )
            thread.start()
            threads.append(thread)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        self.task_queue.join()
        
        # å‘é€ç»“æŸä¿¡å·
        for _ in range(self.max_workers):
            self.task_queue.put(None)
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        for thread in threads:
            thread.join(timeout=5)
        
        pbar.close()
        
        # è®¡ç®—æ€»æ—¶é—´
        self.stats["total_time"] = time.time() - batch_start_time
        
        # æ‰“å°æ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("å¹¶è¡Œæ‰¹é‡å¤„ç†å®Œæˆ")
        self.logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        self.logger.info(f"æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
        self.logger.info(f"å¤„ç†å¤±è´¥: {self.stats['failed_files']}")
        self.logger.info(f"æ€» token ä½¿ç”¨é‡: {self.stats['total_tokens']}")
        self.logger.info(f"æ€»å¤„ç†æ—¶é—´: {self.stats['total_time']:.2f}ç§’")
        
        # å„æ¨¡å‹ç»Ÿè®¡
        self.logger.info("\nå„æ¨¡å‹å¤„ç†ç»Ÿè®¡:")
        for model_id, model_stat in self.stats["model_stats"].items():
            self.logger.info(
                f"  {model_id}: "
                f"æˆåŠŸ {model_stat['processed']}, "
                f"å¤±è´¥ {model_stat['failed']}, "
                f"Tokens {model_stat['tokens']}"
            )
        
        if self.stats["processed_files"] > 0:
            avg_time = self.stats["total_time"] / self.stats["processed_files"]
            self.logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’/æ–‡ä»¶")
        
        self.logger.info("=" * 80)
        
        return {
            "success": self.stats["failed_files"] == 0,
            "stats": self.stats
        }
    
    def save_results(self, output_file: Optional[Union[str, Path]] = None):
        """
        ä¿å­˜å¤„ç†ç»“æœåˆ° JSON æ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
        """
        if output_file is None:
            output_file = self.output_dir / f"parallel_batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            output_file = Path(output_file)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="GraphGen å¹¶è¡Œæ‰¹é‡å¤„ç†è„šæœ¬ - æ”¯æŒå¤šæ¨¡å‹å¹¶è¡Œå¤„ç†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

  # ä½¿ç”¨å¤šä¸ªæ¨¡å‹å¹¶è¡Œå¤„ç†
  python batch_process_parallel.py -i file1.txt file2.txt -c config.json

  # ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆåŒ…å«å¤šä¸ªæ¨¡å‹é…ç½®ï¼‰
  python batch_process_parallel.py -c config.json -b 2

  # ä»æ–‡ä»¶åˆ—è¡¨æ‰¹é‡å¤„ç†
  python batch_process_parallel.py -l file_list.txt -c config.json --batch-size 3
        """
    )
    
    # è¾“å…¥å‚æ•°ç»„
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("-i", "--input-files", nargs='+', help="è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    input_group.add_argument("-l", "--file-list", help="åŒ…å«æ–‡ä»¶è·¯å¾„åˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶")
    input_group.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„ (JSON æ ¼å¼)")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("-m", "--models-config", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ (JSON æ ¼å¼ï¼ŒåŒ…å«æ¨¡å‹åˆ—è¡¨)")
    
    # å¹¶è¡Œé…ç½®
    parser.add_argument("-b", "--batch-size", type=int, default=1,
                      help="æ¯ä¸ªæ¨¡å‹åŒæ—¶å¤„ç†çš„ä»»åŠ¡æ•° (é»˜è®¤: 1)")
    parser.add_argument("-w", "--max-workers", type=int, default=None,
                      help="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: æ¨¡å‹æ•°é‡ * batch_size)")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("-o", "--output-dir", default="tasks/outputs",
                      help="è¾“å‡ºç›®å½• (é»˜è®¤: tasks/outputs)")
    
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # åŠ è½½æ¨¡å‹é…ç½®
    if args.models_config:
        with open(args.models_config, 'r', encoding='utf-8') as f:
            models_data = json.load(f)
            if isinstance(models_data, list):
                model_configs = models_data
            elif isinstance(models_data, dict) and "models" in models_data:
                model_configs = models_data["models"]
            else:
                print("âŒ é”™è¯¯: æ¨¡å‹é…ç½®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                sys.exit(1)
    else:
        # é»˜è®¤ä½¿ç”¨å•ä¸ªæ¨¡å‹ï¼ˆä»ç¯å¢ƒå˜é‡ï¼‰
        model_configs = [{
            "api_key": os.getenv("SYNTHESIZER_API_KEY", ""),
            "synthesizer_url": os.getenv("SYNTHESIZER_BASE_URL", "https://api.huiyan-ai.cn/v1"),
            "synthesizer_model": os.getenv("SYNTHESIZER_MODEL", "gpt-4.1-mini-2025-04-14"),
        }]
    
    if not model_configs:
        print("âŒ é”™è¯¯: æœªæä¾›æ¨¡å‹é…ç½®")
        sys.exit(1)
    
    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if args.input_files:
        file_paths = args.input_files
    elif args.file_list:
        with open(args.file_list, 'r', encoding='utf-8') as f:
            file_paths = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    elif args.config:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
            file_paths = config.get("file_paths", [])
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨å®ƒ
            if "models" in config:
                model_configs = config["models"]
    else:
        print("âŒ é”™è¯¯: æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶")
        sys.exit(1)
    
    # åˆ›å»ºå¹¶è¡Œå¤„ç†å™¨
    processor = ParallelBatchProcessor(
        model_configs=model_configs,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        max_workers=args.max_workers,
        output_data_type=os.getenv("OUTPUT_DATA_TYPE", "aggregated"),
        output_data_format=os.getenv("OUTPUT_DATA_FORMAT", "Alpaca"),
    )
    
    # å¤„ç†æ–‡ä»¶
    result = processor.process_batch(file_paths)
    
    # ä¿å­˜ç»“æœ
    processor.save_results()
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š å¹¶è¡Œæ‰¹é‡å¤„ç†æ€»ç»“:")
    print(f"   æ€»æ–‡ä»¶æ•°: {result['stats']['total_files']}")
    print(f"   æˆåŠŸå¤„ç†: {result['stats']['processed_files']}")
    print(f"   å¤„ç†å¤±è´¥: {result['stats']['failed_files']}")
    print(f"   æ€» token ä½¿ç”¨é‡: {result['stats']['total_tokens']}")
    print(f"   æ€»å¤„ç†æ—¶é—´: {result['stats']['total_time']:.2f}ç§’")
    print("=" * 60)
    
    sys.exit(0 if result["success"] else 1)


if __name__ == "__main__":
    main()

