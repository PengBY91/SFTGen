#!/usr/bin/env python3
"""
GraphGen æ‰¹é‡å¤„ç†è„šæœ¬
æä¾›ç®€å•çš„ Python API æ¥æ‰¹é‡å¤„ç†æ•°æ®ç”Ÿæˆä»»åŠ¡
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from graphgen_cli import GraphGenCLI


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ç±»"""
    
    def __init__(
        self,
        api_key: str,
        synthesizer_url: str = "https://api.huiyan-ai.cn/v1",
        synthesizer_model: str = "gpt-4.1-mini-2025-04-14",
        trainee_url: Optional[str] = None,
        trainee_model: Optional[str] = None,
        trainee_api_key: Optional[str] = None,
        use_trainee_model: bool = False,
        output_dir: str = "tasks/outputs",
        log_dir: str = "logs",
        **kwargs
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            api_key: API Key (å¿…éœ€)
            synthesizer_url: Synthesizer API URL
            synthesizer_model: Synthesizer æ¨¡å‹åç§°
            trainee_url: Trainee API URL (å¦‚æœä½¿ç”¨ trainee æ¨¡å‹)
            trainee_model: Trainee æ¨¡å‹åç§° (å¦‚æœä½¿ç”¨ trainee æ¨¡å‹)
            trainee_api_key: Trainee API Key (å¦‚æœä½¿ç”¨ trainee æ¨¡å‹)
            use_trainee_model: æ˜¯å¦ä½¿ç”¨ Trainee æ¨¡å‹
            output_dir: è¾“å‡ºç›®å½•
            log_dir: æ—¥å¿—ç›®å½•
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
                - output_data_type: è¾“å‡ºæ•°æ®ç±»å‹ï¼Œå¯é€‰ "atomic", "multi_hop", "aggregated", "all"
                  "all" æ¨¡å¼ä¼šåŒæ—¶ä½¿ç”¨æ‰€æœ‰å››ç§ç”Ÿæˆæ¨¡å¼ï¼ˆatomic, aggregated, multi_hop, cotï¼‰ï¼Œ
                  ç”Ÿæˆæ›´å¤šæ•°æ®ä½†æ¶ˆè€—æ›´å¤š token
        """
        self.api_key = api_key
        self.synthesizer_url = synthesizer_url
        self.synthesizer_model = synthesizer_model
        self.trainee_url = trainee_url or "https://api.siliconflow.cn/v1"
        self.trainee_model = trainee_model or "Qwen/Qwen2.5-7B-Instruct"
        self.trainee_api_key = trainee_api_key or api_key
        self.use_trainee_model = use_trainee_model
        self.output_dir = Path(output_dir)
        self.log_dir = Path(log_dir)
        
        # åˆ›å»ºè¾“å‡ºå’Œæ—¥å¿—ç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜è®¤é…ç½®å‚æ•°
        self.config = {
            "chunk_size": kwargs.get("chunk_size", 1024),
            "chunk_overlap": kwargs.get("chunk_overlap", 100),
            "tokenizer": kwargs.get("tokenizer", "cl100k_base"),
            "output_data_type": kwargs.get("output_data_type", "aggregated"),  # å¯é€‰: atomic, multi_hop, aggregated, all
            "output_data_format": kwargs.get("output_data_format", "Alpaca"),
            "quiz_samples": kwargs.get("quiz_samples", 2),
            "bidirectional": kwargs.get("bidirectional", True),
            "expand_method": kwargs.get("expand_method", "max_tokens"),
            "max_extra_edges": kwargs.get("max_extra_edges", 5),
            "max_tokens": kwargs.get("max_tokens", 256),
            "max_depth": kwargs.get("max_depth", 2),
            "edge_sampling": kwargs.get("edge_sampling", "max_loss"),
            "isolated_node_strategy": kwargs.get("isolated_node_strategy", "ignore"),
            "loss_strategy": kwargs.get("loss_strategy", "only_edge"),
            "rpm": kwargs.get("rpm", 1000),
            "tpm": kwargs.get("tpm", 50000),
        }
        
        # åˆå§‹åŒ– CLI
        self.cli = GraphGenCLI()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "total_tokens": 0,
            "total_time": 0,
            "file_results": []
        }
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.log_dir / f"batch_process_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"æ‰¹é‡å¤„ç†æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    def process_file(
        self,
        file_path: Union[str, Path],
        output_file: Optional[Union[str, Path]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å« success, output_file, tokens_used, processing_time ç­‰ä¿¡æ¯
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            self.logger.error(error_msg)
            return {
                "success": False,
                "file_path": str(file_path),
                "error_msg": error_msg
            }
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_file is None:
            base_name = file_path.stem
            output_file = self.output_dir / f"{base_name}_graphgen_output.jsonl"
        else:
            output_file = Path(output_file)
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºå‘½åç©ºé—´å¯¹è±¡æ¥æ¨¡æ‹Ÿ argparse.Namespace
            class Args:
                pass
            
            args = Args()
            args.use_trainee_model = self.use_trainee_model
            args.synthesizer_url = self.synthesizer_url
            args.synthesizer_model = self.synthesizer_model
            args.trainee_url = self.trainee_url
            args.trainee_model = self.trainee_model
            args.api_key = self.api_key
            args.trainee_api_key = self.trainee_api_key
            args.input_file = str(file_path)
            args.output_file = str(output_file)
            
            # è®¾ç½®é…ç½®å‚æ•°
            for key, value in self.config.items():
                setattr(args, key, value)
            
            # å¤„ç†æ–‡ä»¶
            self.logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            result = self.cli.process_single_file(str(file_path), args)
            
            # å¦‚æœæˆåŠŸï¼Œç§»åŠ¨è¾“å‡ºæ–‡ä»¶åˆ°æŒ‡å®šä½ç½®
            if result["success"] and result["output_file"]:
                if Path(result["output_file"]).exists() and result["output_file"] != str(output_file):
                    import shutil
                    shutil.move(result["output_file"], output_file)
                    result["output_file"] = str(output_file)
            
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            
            if result["success"]:
                self.logger.info(
                    f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_path} -> {result['output_file']} "
                    f"(tokens: {result['tokens_used']}, æ—¶é—´: {processing_time:.2f}ç§’)"
                )
            else:
                self.logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {file_path} - {result.get('error_msg', 'æœªçŸ¥é”™è¯¯')}")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            self.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {file_path} - {error_msg}")
            return {
                "success": False,
                "file_path": str(file_path),
                "error_msg": error_msg,
                "processing_time": processing_time
            }
    
    def process_batch(
        self,
        file_paths: List[Union[str, Path]],
        output_dir: Optional[Union[str, Path]] = None,
        continue_on_error: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½• (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„ output_dir)
            continue_on_error: é‡åˆ°é”™è¯¯æ—¶æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œæ¯ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœ
        """
        if output_dir:
            self.output_dir = Path(output_dir)
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        batch_start_time = time.time()
        self.stats = {
            "total_files": len(file_paths),
            "processed_files": 0,
            "failed_files": 0,
            "total_tokens": 0,
            "total_time": 0,
            "file_results": []
        }
        
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹æ‰¹é‡å¤„ç†")
        self.logger.info(f"æ–‡ä»¶æ•°é‡: {len(file_paths)}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info("=" * 80)
        
        # æµ‹è¯• API è¿æ¥
        self.logger.info("æµ‹è¯• API è¿æ¥...")
        if not self.cli.test_api_connection(self.synthesizer_url, self.api_key, self.synthesizer_model):
            self.logger.error("Synthesizer API è¿æ¥å¤±è´¥")
            return {
                "success": False,
                "error": "API è¿æ¥å¤±è´¥",
                "stats": self.stats
            }
        
        if self.use_trainee_model:
            if not self.cli.test_api_connection(self.trainee_url, self.trainee_api_key, self.trainee_model):
                self.logger.error("Trainee API è¿æ¥å¤±è´¥")
                return {
                    "success": False,
                    "error": "Trainee API è¿æ¥å¤±è´¥",
                    "stats": self.stats
                }
        
        # å¤„ç†æ–‡ä»¶
        with tqdm(total=len(file_paths), desc="æ‰¹é‡å¤„ç†", unit="æ–‡ä»¶") as pbar:
            for file_path in file_paths:
                result = self.process_file(file_path)
                self.stats["file_results"].append(result)
                
                if result["success"]:
                    self.stats["processed_files"] += 1
                    self.stats["total_tokens"] += result.get("tokens_used", 0)
                else:
                    self.stats["failed_files"] += 1
                    if not continue_on_error:
                        self.logger.error("é‡åˆ°é”™è¯¯ï¼Œåœæ­¢æ‰¹é‡å¤„ç†")
                        break
                
                pbar.update(1)
                pbar.set_postfix({
                    "æˆåŠŸ": self.stats["processed_files"],
                    "å¤±è´¥": self.stats["failed_files"]
                })
        
        # è®¡ç®—æ€»æ—¶é—´
        self.stats["total_time"] = time.time() - batch_start_time
        
        # æ‰“å°æ€»ç»“
        self.logger.info("=" * 80)
        self.logger.info("æ‰¹é‡å¤„ç†å®Œæˆ")
        self.logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        self.logger.info(f"æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
        self.logger.info(f"å¤„ç†å¤±è´¥: {self.stats['failed_files']}")
        self.logger.info(f"æ€» token ä½¿ç”¨é‡: {self.stats['total_tokens']}")
        self.logger.info(f"æ€»å¤„ç†æ—¶é—´: {self.stats['total_time']:.2f}ç§’")
        if self.stats["processed_files"] > 0:
            avg_time = self.stats["total_time"] / self.stats["processed_files"]
            self.logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’/æ–‡ä»¶")
        self.logger.info("=" * 80)
        
        return {
            "success": self.stats["failed_files"] == 0,
            "stats": self.stats
        }
    
    def process_from_list(
        self,
        list_file: Union[str, Path],
        output_dir: Optional[Union[str, Path]] = None,
        continue_on_error: bool = True
    ) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶åˆ—è¡¨ä¸­è¯»å–æ–‡ä»¶è·¯å¾„å¹¶æ‰¹é‡å¤„ç†
        
        Args:
            list_file: åŒ…å«æ–‡ä»¶è·¯å¾„åˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶ (æ¯è¡Œä¸€ä¸ªè·¯å¾„)
            output_dir: è¾“å‡ºç›®å½•
            continue_on_error: é‡åˆ°é”™è¯¯æ—¶æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœå­—å…¸
        """
        list_file = Path(list_file)
        
        if not list_file.exists():
            error_msg = f"æ–‡ä»¶åˆ—è¡¨ä¸å­˜åœ¨: {list_file}"
            self.logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg
            }
        
        # è¯»å–æ–‡ä»¶åˆ—è¡¨
        with open(list_file, 'r', encoding='utf-8') as f:
            file_paths = [
                line.strip() for line in f 
                if line.strip() and not line.startswith('#')
            ]
        
        self.logger.info(f"ä»æ–‡ä»¶åˆ—è¡¨è¯»å–åˆ° {len(file_paths)} ä¸ªæ–‡ä»¶è·¯å¾„")
        
        return self.process_batch(file_paths, output_dir, continue_on_error)
    
    def process_directory(
        self,
        directory: Union[str, Path],
        pattern: str = "*.txt",
        output_dir: Optional[Union[str, Path]] = None,
        continue_on_error: bool = True,
        recursive: bool = False
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰åŒ¹é…æ–‡ä»¶
        
        Args:
            directory: ç›®å½•è·¯å¾„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼ (ä¾‹å¦‚ "*.txt", "*.json")
            output_dir: è¾“å‡ºç›®å½•
            continue_on_error: é‡åˆ°é”™è¯¯æ—¶æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
            recursive: æ˜¯å¦é€’å½’æœç´¢å­ç›®å½•
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœå­—å…¸
        """
        directory = Path(directory)
        
        if not directory.exists():
            error_msg = f"ç›®å½•ä¸å­˜åœ¨: {directory}"
            self.logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg
            }
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        if recursive:
            file_paths = list(directory.rglob(pattern))
        else:
            file_paths = list(directory.glob(pattern))
        
        # è¿‡æ»¤å‡ºæ–‡ä»¶ (æ’é™¤ç›®å½•)
        file_paths = [f for f in file_paths if f.is_file()]
        
        # è¿‡æ»¤æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        valid_extensions = ['.txt', '.json', '.jsonl', '.csv']
        file_paths = [
            f for f in file_paths 
            if f.suffix.lower() in valid_extensions
        ]
        
        self.logger.info(f"åœ¨ç›®å½• {directory} ä¸­æ‰¾åˆ° {len(file_paths)} ä¸ªåŒ¹é…æ–‡ä»¶")
        
        return self.process_batch(file_paths, output_dir, continue_on_error)
    
    def save_results(self, output_file: Optional[Union[str, Path]] = None):
        """
        ä¿å­˜å¤„ç†ç»“æœåˆ° JSON æ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)
        """
        if output_file is None:
            output_file = self.output_dir / f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            output_file = Path(output_file)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="GraphGen æ‰¹é‡å¤„ç†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

  # å¤„ç†å•ä¸ªæ–‡ä»¶
  python batch_process.py -i input.txt -k your_api_key

  # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
  python batch_process.py -b file1.txt file2.json file3.csv -k your_api_key

  # ä»æ–‡ä»¶åˆ—è¡¨æ‰¹é‡å¤„ç†
  python batch_process.py -l file_list.txt -k your_api_key

  # å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
  python batch_process.py -d data/ -k your_api_key

  # ä½¿ç”¨é…ç½®æ–‡ä»¶
  python batch_process.py -c config.json -b file1.txt file2.txt
        """
    )
    
    # è¾“å…¥å‚æ•°ç»„ - äº’æ–¥é€‰æ‹©
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("-i", "--input-file", help="å•ä¸ªè¾“å…¥æ–‡ä»¶è·¯å¾„")
    input_group.add_argument("-b", "--batch-files", nargs='+', help="æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶è·¯å¾„")
    input_group.add_argument("-l", "--file-list", help="åŒ…å«æ–‡ä»¶è·¯å¾„åˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶")
    input_group.add_argument("-d", "--directory", help="å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰åŒ¹é…æ–‡ä»¶")
    input_group.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„ (JSON æ ¼å¼)")
    
    # API é…ç½®
    parser.add_argument("-k", "--api-key", 
                      default=os.getenv("SYNTHESIZER_API_KEY", ""),
                      help="API Key (é»˜è®¤ä»ç¯å¢ƒå˜é‡ SYNTHESIZER_API_KEY è¯»å–)")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("-o", "--output-dir", 
                      default="tasks/outputs",
                      help="è¾“å‡ºç›®å½• (é»˜è®¤: tasks/outputs)")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--use-trainee-model", action="store_true", 
                      help="ä½¿ç”¨ Trainee æ¨¡å‹")
    parser.add_argument("--synthesizer-url", 
                      default=os.getenv("SYNTHESIZER_BASE_URL", "https://api.huiyan-ai.cn/v1"),
                      help="Synthesizer API URL")
    parser.add_argument("--synthesizer-model", 
                      default=os.getenv("SYNTHESIZER_MODEL", "gpt-4.1-mini-2025-04-14"),
                      help="Synthesizer æ¨¡å‹åç§°")
    
    # å…¶ä»–é…ç½®å‚æ•°
    parser.add_argument("--chunk-size", type=int, default=1024, help="æ–‡æœ¬å—å¤§å°")
    parser.add_argument("--chunk-overlap", type=int, default=100, help="æ–‡æœ¬å—é‡å å¤§å°")
    parser.add_argument("--max-tokens", type=int, default=256, help="æœ€å¤§ token æ•°")
    parser.add_argument("--max-depth", type=int, default=2, help="æœ€å¤§æ·±åº¦")
    
    # ç›®å½•å¤„ç†é€‰é¡¹
    parser.add_argument("--pattern", default="*.txt", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼ (ç”¨äº -d é€‰é¡¹)")
    parser.add_argument("--recursive", action="store_true", help="é€’å½’æœç´¢å­ç›®å½• (ç”¨äº -d é€‰é¡¹)")
    
    args = parser.parse_args()
    
    # éªŒè¯ API Key
    if not args.api_key:
        print("âŒ é”™è¯¯: æœªæä¾› API Key")
        print("è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æä¾› API Key:")
        print("1. å‘½ä»¤è¡Œå‚æ•°: -k your_api_key")
        print("2. ç¯å¢ƒå˜é‡: SYNTHESIZER_API_KEY=your_api_key")
        sys.exit(1)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶
    if args.config:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ä»é…ç½®æ–‡ä»¶åˆ›å»ºå¤„ç†å™¨
        processor = BatchProcessor(**config)
        
        # å¤„ç†æ–‡ä»¶
        if "file_paths" in config:
            result = processor.process_batch(config["file_paths"])
        elif "list_file" in config:
            result = processor.process_from_list(config["list_file"])
        elif "directory" in config:
            result = processor.process_directory(
                config["directory"],
                pattern=config.get("pattern", "*.txt"),
                recursive=config.get("recursive", False)
            )
        else:
            print("âŒ é…ç½®æ–‡ä»¶ä¸­å¿…é¡»åŒ…å« file_paths, list_file æˆ– directory ä¹‹ä¸€")
            sys.exit(1)
    else:
        # åˆ›å»ºå¤„ç†å™¨
        processor = BatchProcessor(
            api_key=args.api_key,
            synthesizer_url=args.synthesizer_url,
            synthesizer_model=args.synthesizer_model,
            use_trainee_model=args.use_trainee_model,
            output_dir=args.output_dir,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap,
            max_tokens=args.max_tokens,
            max_depth=args.max_depth,
        )
        
        # å¤„ç†æ–‡ä»¶
        if args.input_file:
            result = processor.process_file(args.input_file)
        elif args.batch_files:
            result = processor.process_batch(args.batch_files)
        elif args.file_list:
            result = processor.process_from_list(args.file_list)
        elif args.directory:
            result = processor.process_directory(
                args.directory,
                pattern=args.pattern,
                recursive=args.recursive
            )
    
    # ä¿å­˜ç»“æœ
    processor.save_results()
    
    # é€€å‡º
    if result.get("success", False):
        print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        sys.exit(0)
    else:
        print("ğŸ’¥ æ‰¹é‡å¤„ç†å®Œæˆï¼Œä½†æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()

